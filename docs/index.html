<!DOCTYPE html>
<html lang="de">
<head>
<meta charset="utf-8"/>
<title>Backpropagation</title>
<meta name="author" content="Lucas Sas Brunschier"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="reveal/css/reveal.css"/>

<link rel="stylesheet" href="reveal/css/theme/solarized.css" id="theme"/>

<link rel="stylesheet" href="style.css"/>

<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'reveal/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<h1 class="title">Backpropagation</h1><h2 class="author">Lucas Sas Brunschier</h2><h2 class="date">SS20</h2>
</section>

<section>
<section id="slide-org1896fe0">
<h2 id="org1896fe0">Intro</h2>
<p>
Präsentation ist auch online <a href="https://sirbubbls.github.io/backpropagation-seminar">sirbubbls.github.io/backpropagation-seminar</a>
</p>

<p>
Präsentation ist in Org-Mode geschrieben, also sourcen aller Grafiken und
Beispiele sind integriert.
</p>
</section>
<section id="slide-orgc58ef53">
<h3 id="orgc58ef53">Zusätzliche Ressourcen</h3>
<p>
Deep Learning (Ian Goodfellow, Yoshua Bengio &amp; Aaron Courville)
</p>

<div class="figure">
<p><img src="https://images-eu.ssl-images-amazon.com/images/I/610HnULa0dL._SY445_QL70_ML2_.jpg" alt="610HnULa0dL._SY445_QL70_ML2_.jpg" width="150" />
</p>
</div>

<p>
<a href="https://www.deeplearningbook.org">https://www.deeplearningbook.org</a>
</p>

<p>
<a href="https://www.deeplearningbook.org/contents/mlp.html">Backpropagation Kapitel</a>
</p>

</section>
<section id="slide-orgac8103a">
<h3 id="orgac8103a">Jupyter Notebook</h3>
<p>
Beispiele für alle Methoden dieser Präsentation sind in diesem <a href="https://github.com/SirBubbls/backpropagation-seminar/blob/master/Backpropagation.ipynb">IPython Notebook</a> zu finden.
</p>
</section>
</section>
<section>
<section id="slide-orgddfe9b1">
<h2 id="orgddfe9b1">Inhalt</h2>
<ol>
<li>Neuronale Netze</li>
<li>Vorwissen</li>
<li>Forward Propagation</li>
<li>Backpropagation</li>
<li>General Backpropagation</li>

</ol>

</section>
</section>
<section>
<section id="slide-org9436b38">
<h2 id="org9436b38">Neuronale Netze</h2>
<p>
Formale Definition für ein neuronales Netz: \(y=f(x; \theta)\) und \(y=f^*(x)\)
</p>
<ul>
<li>\(y\) ist den Wert den unser NN vorraussagen soll</li>
<li>\(x\) sind die Input Daten, die das NN erhält</li>
<li>\(\theta\) sind Parameter des neuronalen Netzes, um \(f\) so nah wie möglich an
die optimale Funktion \(f^*\) anzunähern.</li>

</ul>
</section>
<section id="slide-org1404db1">
<h3 id="org1404db1">Wie ist nun ein neurales Netzwerk aufgebaut?</h3>
<p>
Wir teilen das Netzwerk in Schichten (Layer) auf.
</p>


<div class="figure">
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3d/Neural_network.svg/1200px-Neural_network.svg.png" alt="1200px-Neural_network.svg.png" width="50%" height="50%" />
</p>
</div>

<p>
Jeder Layer bildet eine Funktion \(f^{i}\), mit \(i=Layer\ Index\) ab.
</p>

</section>
<section id="slide-org86a7a1c">
<h4 id="org86a7a1c">Formell</h4>
<p>
Somit ist ein neurales Netzwerk eine Kette an Funktionen \(f\).
</p>
<blockquote>
<p>
Ein Netz mit \(3\) Layern wäre somit \(f^2(f^1(f^0(X)))\)
mit \(X=Input\ Data\)
</p>
</blockquote>
</section>
<section id="slide-orgeb562f4">
<h4 id="orgeb562f4">Aufbau eines Layers</h4>
<p>
Jeder Layer enthält mindestens folgende Informationen:
</p>
<ul>
<li>Eine Weight Matrix (\(w\))</li>
<li>Einen Bias Vektor (\(b\))</li>

</ul>
</section>
<section id="slide-org5e6c22a">
<h4 id="org5e6c22a">Aktivierungsfunktion</h4>
<p>
Da wir bei Neural Networks oft versuchen non-lineare Zusammenhänge zu aproximieren, benötigen wir auch eine nicht-lineare Komponente in unserem NN.
</p>
</section>
<section id="slide-org5b56834">
<h4 id="org5b56834">Beliebte Aktivierungsfunktionen</h4>
<ul>
<li>Rectified Linear Unit (\(ReLU\))</li>
<li>\(Leaky\ ReLU\)</li>
<li>Sigmoid Function</li>

</ul>
</section>
<section id="slide-orga9d000a">
<h4 id="orga9d000a">Cost Function (\(J\))</h4>
<p>
Eine Funktion um zu bestimmen wie &rsquo;nah&rsquo; wir uns an unserem erwarteten Inference Wert befinden.
</p>
<blockquote>
<p>
In dieser Präsentation benutzen wir die Euklidean-Distance \((x-y)^2\) als Cost Function.
</p>
</blockquote>
</section>
</section>
<section>
<section id="slide-orgc6d6d2a">
<h2 id="orgc6d6d2a">Historisches</h2>
<p>
Erste Verwendung in Zusammenhang mit artificial neural networks im Jahre 1985 im Buch &rsquo;Learning Logic&rsquo; (Parker, D.B.).
</p>

</section>
</section>
<section>
<section id="slide-orgad10dd9">
<h2 id="orgad10dd9">Vorwissen</h2>
<div class="outline-text-2" id="text-orgad10dd9">
</div>
</section>
<section id="slide-orgf6869bb">
<h3 id="orgf6869bb">Gradient</h3>
<p>
Der Gradient ist der Vektor aller partiellen Ableitungen einer Funktion \(f\).
</p>
<blockquote>
<p>
Notation: \(\nabla_xf(x)\)
</p>
</blockquote>
</section>
<section id="slide-org78a0b02">
<h4 id="org78a0b02">Beispiel</h4>
<blockquote>
<p>
\(f(x, y) = 2x^2 + y^3\)
</p>
</blockquote>
<p>
\(\rightarrow \nabla_xf(x)=\left(\begin{array}{c} f'_x \\ f'_y \end{array}\right)= \left(\begin{array}{c} 4x \\ 3y \end{array}\right)\)
</p>
</section>
<section id="slide-org63e7abf">
<h3 id="org63e7abf">Stochastic Gradient Descent</h3>
<p>
Der Gradient Descent Algorithmus wird dafür verwendet ein lokales Minimum einer Funktion zu bestimmen.
</p>
</section>
<section id="slide-orge5ee1f4">
<h4 id="orge5ee1f4">Beispiel</h4>
<p>
Funktion \(f(x)=x_1^2-x_2^2\) ist gegeben. <br />
Also: \(\nabla_xf(x)=\left(\begin{array}{c} f'_{x_1} \\ f'_{x_2} \end{array}\right)= \left(\begin{array}{c} 2x_1 \\ -2x_2 \end{array}\right)\) <br />
Wir starten mit einem beliebigen Punkt: z.B. \(\left(\begin{array}{c} 2 \\ 1 \end{array}\right)\) und setzen ein: <br />
\(\left(\begin{array}{c} 2x_1 \\ -2x_2 \end{array}\right) = \left(\begin{array}{c} 2 * 2 \\ -2 * 1 \end{array}\right) = \left(\begin{array}{c} 4 \\ -2 \end{array}\right)\)
</p>

<p>
\(Neuer\ Punkt = \left(\begin{array}{c} 2 \\ 1 \end{array}\right)+ \lambda \left(\begin{array}{c} 4 \\ -2 \end{array}\right)\) mit \(\lambda: learning\ rate\)
</p>

</section>
<section id="slide-orgf64acf2">
<h3 id="orgf64acf2">Computational Graphs</h3>
<aside class="notes">
<p>
Rechenoperationen in ANN&rsquo;s werden typischerweise nicht in mathematischen Formeln angegeben, sondern in sog. computational Graphs.
</p>

</aside>

<p>
Typischerweise werden Operationen in artificial neural networks nicht mit mathematischen Formeln angegeben, sondern als Graph dargestellt.
</p>

</section>
<section id="slide-org77f3adc">
<h4 id="org77f3adc">Repräsentation</h4>
<p>
Jede Node in einem Graph \(G\) repräsentiert eine mathematische Operation.<br />
</p>

<p>
Beispielsweise:
</p>
<ul>
<li>Matrix Multiplikation</li>
<li>Addition</li>
<li>Skalare Multiplikation</li>

</ul>
</section>
<section id="slide-org47231cd">
<h4 id="org47231cd">Addition Beispiel</h4>
<p>
\[
y = a+b
\]
</p>


<div class="figure">
<p><img src="./basic_graph.png" alt="basic_graph.png" />
</p>
</div>
</section>
<section id="slide-orgfa17de3">
<h4 id="orgfa17de3">Komplexere Beispiele</h4>
<p>
\(x=y+z\)
\(a=x\odot z\)
</p>


<div class="figure">
<p><img src="./basic_graph_2.png" alt="basic_graph_2.png" />
</p>
</div>

</section>
</section>
<section>
<section id="slide-org7a69204">
<h2 id="org7a69204">Forward Propagation</h2>
<p>
Ein Layer in einem Feed-Forward Neural Network besteht aus folgenden Elementen:
</p>
<ul>
<li>Inputs (\(X\))</li>
<li>Weights (\(W\))</li>
<li>Biases (\(b\))</li>
<li>Output (\(a\))</li>

</ul>
</section>
<section id="slide-org66e9fcc">
<h3 id="org66e9fcc">Formell</h3>
<p>
Um die Aktivierungen (\(a\)) eines Layers zu berechen können wir folgende Formel benutzen:
</p>
<blockquote>
<p>
\(a_L = \sigma(a_{L-1} w_L + b_L)\)
</p>
</blockquote>
<p>
Der berechnete Vektor \(a_L\) dient dem Layer \(L+1\) als Input.
</p>
</section>
<section id="slide-orgc186874">
<h3 id="orgc186874">Computational Graph</h3>
<aside class="notes">
<p>
Backpropagation besteht letztendlich nur darin den Graph in die andere Richtung zu propagieren.
</p>

</aside>

<p>
\[
a = \sigma(a_{L-1}w_L+b)
\]
</p>


<div class="figure">
<p><img src="./forward_prop_graph.png" alt="forward_prop_graph.png" />
</p>
</div>

</section>
<section id="slide-orgdb162e1">
<h3 id="orgdb162e1">Beispiel (XOR)</h3>
<p>
\(W=\left[\begin{array}{ccc} 1 & 1 \\ 1 & 1 \end{array}\right]\) <br />
\(c=\left [\begin{array}{ccc} 0 \\ -1 \end{array} \right]\) <br />
</p>
</section>
<section id="slide-org1c68970">
<h3 id="org1c68970">Multiplizieren der Weights (\(W\)) und Inputs (\(X\))</h3>
<p>
\[
XW=\left[\begin{array}{ccc} 0 & 0 \\ 0 & 1 \\ 1 & 0 \\ 1 & 1 \end{array} \right]
\left[\begin{array}{ccc} 1 & 1 \\ 1 & 1 \end{array}\right]=
\left[\begin{array}{ccc} 0 & 0 \\ 1 & 1 \\ 1 & 1 \\ 2 & 2 \end{array} \right]
\]
</p>

</section>
<section id="slide-orgcdadc31">
<h3 id="orgcdadc31">Addieren des Bias Vektors (\(c\))</h3>
<p>
\[
XW + c=
\left[\begin{array}{ccc} 0 & 0 \\ 1 & 1 \\ 1 & 1 \\ 2 & 2 \end{array} \right] +
\left(\begin{array}{ccc} 0 \\ -1 \end{array}\right)=
\left[\begin{array}{ccc} 0 & -1 \\ 1 & 0 \\ 1 & 0 \\ 2 & 1 \end{array} \right]
\]
</p>
</section>
<section id="slide-org823d8ea">
<h3 id="org823d8ea">Aktivierungsfunktion (in diesem Fall \(ReLU\))</h3>
<blockquote>
<p>
\(ReLU:= f(x)=max(0, x)\)
</p>
</blockquote>
<p>
\[
relu(XW+c)=
relu(\left[\begin{array}{ccc} 0 & -1 \\ 1 & 0 \\ 1 & 0 \\ 2 & 1 \end{array} \right])=
\left[\begin{array}{ccc} 0 & 0 \\ 1 & 0 \\ 1 & 0 \\ 2 & 1 \end{array} \right]
\]
</p>

<p>
Die Aktivierungsfunktion wird auf jedes Element der Matrix ausgeführt.
</p>

</section>
<section id="slide-orgbb22a2c">
<h3 id="orgbb22a2c">Output Layer</h3>
<p>
Multiplizieren der Output Matrix des ersten Layers mit den Weights des Output Layers (\(w\)).
\[
w= relu(XW+c)* \left[\begin{array}{ccc} 1 \\ -2 \end{array}\right]=
\left[\begin{array}{ccc} 0 & 0 \\ 1 & 0 \\ 1 & 0 \\ 2 & 1 \end{array} \right]*
\left[\begin{array}{ccc} 1 \\ -2 \end{array}\right]=
\left[\begin{array}{ccc} 0 \\ 1 \\ 1 \\ 0 \end{array}\right]
\]
</p>
</section>
<section id="slide-org859d033">
<h3 id="org859d033">Predictions &amp; Input</h3>
<p>
Input: \(\left[\begin{array}{ccc} 0 & 0 \\ 0 & 1 \\ 1 & 0 \\ 1 & 1 \end{array} \right]\)
Predictions: \(\left[\begin{array}{ccc} 0 \\ 1 \\ 1 \\ 0 \end{array}\right]\)
</p>

</section>
<section id="slide-org54d3436">
<h3 id="org54d3436">Code Beispiel</h3>
<div class="org-src-container">

<pre  class="src src-python"><span style="color: #fb2874;">def</span> <span style="color: #b6e63e;">forward</span>(X):
    <span style="color: #fd971f;">a</span> = X
    <span style="color: #fb2874;">for</span> layer <span style="color: #fb2874;">in</span> L:
        <span style="color: #fd971f;">a</span> = h @ L.weights + L.bias
    <span style="color: #fb2874;">return</span> a
</pre>
</div>

</section>
</section>
<section>
<section id="slide-orgec3b114">
<h2 id="orgec3b114">Backpropagation</h2>
<div class="outline-text-2" id="text-orgec3b114">
</div>
</section>
<section id="slide-org076fe1d">
<h3 id="org076fe1d">Wozu brauchen wir den Backpropagation Algorithmus?</h3>
<p>
Ein fundamentaler Baustein, von neuralen Netzen.
</p>

<p>
Backpropagation ist kein Lernalgorithmus/Optimierungsalgorithmus, sondern aussschlißlich für die Generierung der Gradients jedes Layers zuständig.
</p>

<p>
Also suchen wir folgende Gradienten:
</p>
<ul>
<li>\(\nabla_{b^k} J\)</li>
<li>\(\nabla_{w^k} J\)</li>

</ul>

</section>
<section id="slide-org0f9b1e2">
<h3 id="org0f9b1e2">Kettenregel</h3>
<aside class="notes">
<p>
Da ein NN prinzipiell nur viele geschachtelte Funktionen sind ist die Kettenregel sehr nützlich um die Ableitungen für jede Funktion zu bestimmen.
</p>

</aside>

<p>
Die Kettenregel ist nützlich um Ableitungen aus schon bereits vorhandenen Ableitungen zu konstruieren.
</p>

<p>
\[y=g(x)\ und\ z=f(g(x))=f(y)\]
</p>

<p>
Dann besagt die Kettenregel: \(\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}\)
</p>
</section>
<section id="slide-org1b73727">
<h3 id="org1b73727">Kettenregel als Graph</h3>
<aside class="notes">
<p>
An der Formel \(f'(f(f(w)))f'(f(w))f'(w)\) erkennt man, dass immer die Zwischenergebnisse aus jedem Schritt benötigt werden um die Korrekte Ableitung \(\frac{\partial z}{\partial w}\).
</p>

</aside>

<p>
\[
x = f(w),\ y=f(x),\ z=f(y)
\]
</p>


<div class="figure">
<p><img src="./chain_rule_derriv.jpg" alt="chain_rule_derriv.jpg" />
</p>
</div>

<p>
\[
\frac{\partial z}{\partial w}=
\frac{\partial z}{\partial y}
\frac{\partial y}{\partial x}
\frac{\partial x}{\partial w}
=
f'(y)f'(x)f'(w) \\
= f'(f(f(w)))f'(f(w))f'(w)
\]
</p>

</section>
<section id="slide-org2186563">
<h3 id="org2186563">Anpassung der Forward Propagation</h3>
<aside class="notes">
<p>
Wie davor gezeigt müssen wir nun Zwischenergebnisse aus der Forward Progagation speichern um im Anschluss effizient die Backpropagation durchführen zu können.
Eine Alternative ist bei <b>limitiertem Speicher</b> die Zwischenergebnisse immer neu zu evaluieren, wenn sie benötigt werden. (-&gt; Höhere Laufzeit)
</p>

</aside>

<p>
Wir benötigen folgende Werte aus jedem Layer um den Backpropagation Algorithmus ausführen zu können.
</p>
<ul>
<li>\(a\) Aktivation Vektor</li>
<li>\(z\) Pre Activation Function Vektor</li>

</ul>

<blockquote>
<p>
\(f'(y)f'(x)f'(w)\): Speichern der Zwischenergebnisse in Variablen
\(f'(f(f(w)))f'(f(w))f'(w)\): Neu Evaluierung der Zwischenergebnisse
</p>
</blockquote>
</section>
<section id="slide-org676f52b">
<h3 id="org676f52b">Beschreibung des Algorithmus</h3>
<div class="outline-text-3" id="text-org676f52b">
</div>
</section>
<section id="slide-orgbd12434">
<h4 id="orgbd12434">Schritt 1</h4>
<p>
Forward Propagation ausführen.
</p>
</section>
<section id="slide-org833712c">
<h4 id="org833712c">Schritt 2</h4>
<p>
Wir berechnen den Gradienten der Cost Function \(J\).
\(J = \frac{1}{2} (y-X)^2 \rightarrow \nabla_y J = X - y\)
</p>
</section>
<section id="slide-orgc5414f9">
<h4 id="orgc5414f9">Schritt 3</h4>
<p>
Erst müssen wir den Gradienten in Relation zu den pre activation function values berechnen.
</p>
<blockquote>
<p>
\(\nabla_{a^{k}} J = g \odot f'(a^{(k)})\)
</p>
</blockquote>
<p>
mit \(f'(x) := Ableitung\ der\ Aktivierungsfunktioin\)
</p>
</section>
<section id="slide-orgcd3eea0">
<h4 id="orgcd3eea0">Schritt 4</h4>
<p>
Bias Gradienten berechnen.
</p>
<blockquote>
<p>
\(\nabla_{b^{k}} J = g\)
</p>
</blockquote>
<p>
Weight Gradienten berechnen.
</p>
<blockquote>
<p>
\(\nabla_{w^k} J = ga^{k-1}\)
</p>
</blockquote>
</section>
<section id="slide-orgfc8028d">
<h4 id="orgfc8028d">Schritt 5</h4>
<p>
\(\nabla a^{k-1} J = w^kg\)
</p>
</section>
<section id="slide-org6a997be">
<h3 id="org6a997be">Graph</h3>

<div class="figure">
<p><img src="./backprop_derriv.jpg" alt="backprop_derriv.jpg" />
</p>
</div>
</section>
<section id="slide-orgf017743">
<h3 id="orgf017743">TODO Delta Rule</h3>
<p>
In neural Networks kann der Gradient Descent Algorithmus zu der sog. <b>Delta Rule</b> zusammengefasst werden.
</p>
<aside class="notes">
<p>
\(\lambda\) ist learning rate <br />
\(\alpha\) ist die aktivierungsfunktion <br />
\(z\) ist inputs * weights <br />
</p>

</aside>

<blockquote>
<p>
\[
\nabla w_{ji} = \lambda ( - a) \alpha'(z)a_{L-1}
\]
</p>
</blockquote>
</section>
<section id="slide-org6a0e3a7">
<h3 id="org6a0e3a7">Praktisches Beispiel in Python</h3>
<div class = "stretch">
     <iframe width="100%" height="100%" src="http://localhost:8888/lab"></iframe>
</div>

</section>
<section id="slide-orgc14a89a">
<h3 id="orgc14a89a">Anmerkungen</h3>
<p>
In der Praxis werden keine Vektoren als Input Daten benutzt, sondern Matrizen (siehe <code>XOR</code> Beispiel).
\[
Input = \left[\begin{array}{ccc} 0 & 0 \\ 0 & 1 \\ 1 & 0 \\ 1 & 1 \end{array} \right]
\]
</p>

<p>
Wir erhalten nun auch mehrere Gradienten in Form einer Matrix. Wir können nun den Durchschnitt der Gradienten nutzen um unsere Weights anzupassen.
</p>

</section>
</section>
<section>
<section id="slide-org6283905">
<h2 id="org6283905">General BackPropagation nach Ian Goodfellow</h2>
<p>
Bisher haben wir uns nur mit Backpropagation in Zusammenhang mit neuralen Netzwerken beschäftigt. <br />
Backpropagation kann aber auch generell für andere Anwendungen eingesetzt werden.
</p>
</section>
<section id="slide-orgfc48c45">
<h3 id="orgfc48c45">TODO Operationen</h3>
<p>
Jede Operation wird durch eine Variable repräsentiert. <br />
</p>

</section>
<section id="slide-orga29c799">
<h4 id="orga29c799">Funktionen</h4>
<aside class="notes">
<p>
<code>get_operation</code> Beispiel bei einer Variable, die durch Matrix Multiplikation generiert wird, würde genau diese Operation zurück gegeben werden.
</p>

</aside>

<p>
Folgende Funktionen werden von Operationen implementiert:
</p>
<ul>
<li><code>get_operation()</code></li>
<li><code>get_consumers()</code> <br />
Gibt alle Variablen/Operationen zurück, die &rsquo;Kinder&rsquo; von sich selber sind.</li>
<li><code>get_inputs()</code> <br />
Gibt alle Variablen/Operationen zurück, die &rsquo;Eltern&rsquo; von sich selber sind.</li>
<li><code>bprop()</code> <br />
Muss bei jeder Operation implementiert werden.</li>

</ul>
</section>
<section id="slide-org475d5c9">
<h4 id="org475d5c9">Generalisierbarkeit</h4>
<p>
Dadurch ist der Backpropagation Algorithmus sehr allgemein anwendbar. <br />
</p>

<p>
Jede Operation ist für seine eigene Differenzierung verantwortlich und benötigt keine weiteren Informationen.
</p>

</section>
</section>
<section>
<section id="slide-orgc7f0942">
<h2 id="orgc7f0942">Ableitungen eines höheren Grads</h2>

</section>
</section>
<section>
<section id="slide-org10b305e">
<h2 id="org10b305e">TODO Laufzeit</h2>
<p>
**
</p>
</section>
</section>
<section>
<section id="slide-org70c2c22">
<h2 id="org70c2c22">Quellen</h2>
<ul>
<li>❤️ Deep Learning (Ian Goodfellow, Yoshua Bengio &amp; Aaron Courville)</li>
<li><a href="https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c">https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c</a></li>
<li>Wikipedia: <a href="https://en.wikipedia.org/wiki/Backpropagation">https://en.wikipedia.org/wiki/Backpropagation</a></li>
<li>Wikipedia: <a href="https://en.wikipedia.org/wiki/Delta_rule">https://en.wikipedia.org/wiki/Delta_rule</a></li>

</ul>
</section>
</section>
</div>
</div>
<script src="reveal/lib/js/head.min.js"></script>
<script src="reveal/js/reveal.js"></script>
<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c',
rollingLinks: false,
keyboard: true,
mouseWheel: false,
fragmentInURL: false,
hashOneBasedIndex: false,
pdfSeparateFragments: true,

overview: true,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'convex', // see README of reveal.js for options
transitionSpeed: 'default',

// Optional libraries used to extend reveal.js
dependencies: [
 { src: 'reveal/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'reveal/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'reveal/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'reveal/plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'reveal/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } }]

});
</script>
</body>
</html>
