<!DOCTYPE html>
<html lang="de">
<head>
<meta charset="utf-8"/>
<title>Backpropagation</title>
<meta name="author" content="Lucas Sas Brunschier"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="reveal/css/reveal.css"/>

<link rel="stylesheet" href="reveal/css/theme/solarized.css" id="theme"/>

<link rel="stylesheet" href="style.css"/>

<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'reveal/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<h1 class="title">Backpropagation</h1><h2 class="author">Lucas Sas Brunschier</h2><h2 class="date">SS20</h2>
</section>

<section>
<section id="slide-org884c251">
<h2 id="org884c251">Intro</h2>
<p>
Präsentation ist auch online <a href="https://sirbubbls.github.io/backpropagation-seminar">sirbubbls.github.io/backpropagation-seminar</a>
</p>

<p>
Präsentation ist in Org-Mode geschrieben, also sourcen aller Grafiken und
Beispiele sind integriert.
</p>
</section>
<section id="slide-orgbec82c6">
<h3 id="orgbec82c6">Zusätzliche Ressourcen</h3>
<p>
Deep Learning (Ian Goodfellow, Yoshua Bengio &amp; Aaron Courville)
</p>

<div class="figure">
<p><img src="https://images-eu.ssl-images-amazon.com/images/I/610HnULa0dL._SY445_QL70_ML2_.jpg" alt="610HnULa0dL._SY445_QL70_ML2_.jpg" width="150" />
</p>
</div>

<p>
<a href="https://www.deeplearningbook.org">https://www.deeplearningbook.org</a>
</p>

<p>
<a href="https://www.deeplearningbook.org/contents/mlp.html">Backpropagation Kapitel</a>
</p>

</section>
<section id="slide-org421665b">
<h3 id="org421665b">Jupyter Notebook</h3>
<p>
Beispiele für alle praktischen Beispiele dieser Präsentation sind in diesem <a href="https://github.com/SirBubbls/backpropagation-seminar">Repository</a> zu finden.
</p>

</section>
</section>
<section>
<section id="slide-orgd927d9c">
<h2 id="orgd927d9c">Agenda</h2>
</section>
<section>
<ol>
<li>Vorwissen
<ul>
<li>Gradients &amp; Stochastic Gradient Descent</li>
<li>Computational Graphs</li>

</ul></li>
<li>Neuronale Netze</li>
<li>Forward Propagation</li>
<li>Backpropagation
<ul>
<li>Kettenregel</li>
<li>Backpropagation</li>
<li>Delta Rule</li>
<li>Implementation in Python</li>

</ul></li>
<li>General Backpropagation</li>
<li>Historisches</li>
<li>Quellen</li>

</ol>

</section>
</section>
<section>
<section id="slide-orgcf2daee">
<h2 id="orgcf2daee">Vorwissen</h2>
<div class="outline-text-2" id="text-orgcf2daee">
</div>
</section>
<section id="slide-org447591a">
<h3 id="org447591a">Gradient</h3>
<p>
Der Gradient ist der Vektor aller partiellen Ableitungen einer Funktion \(f\).
</p>
<blockquote>
<p>
Notation: \(\nabla f(x)\)
</p>
</blockquote>

</section>
<section id="slide-org5188cac">
<h4 id="org5188cac">Beispiel</h4>
<blockquote>
<p>
\(f(x) = 2x_1^2 + x_2^3\)
</p>
</blockquote>
<p>
\(\rightarrow \nabla_xf(x)=\left(\begin{array}{c} f'_{x_1} \\ f'_{x_2} \end{array}\right)= \left(\begin{array}{c} 4x_1 \\ 3x^2_2 \end{array}\right)\)
</p>

</section>
<section id="slide-org8b78d20">
<h4 id="org8b78d20">Jacobi Matrix</h4>
<blockquote>
<p>
Bei mehreren Funktionen (o.a. <b>Komponenten Funktionen</b>).
</p>
</blockquote>

<p>
Bei mehreren Funktionen bilden deren Gradienten eine Jacobi Matrix.
</p>

<p>
\[
J(f) =\left (\begin{array}{cc} \nabla f_1 \\ \nabla f_2 \end{array} \right )
\]
</p>

</section>
<section id="slide-org431754f">
<h3 id="org431754f">Stochastic Gradient Descent</h3>
<p>
Der Gradient Descent Algorithmus wird dafür verwendet ein lokales Minimum einer Funktion zu bestimmen.
</p>
</section>
<section id="slide-org0e57526">
<h4 id="org0e57526">Beispiel</h4>
<p>
Funktion \(f(x)=x_1^2-x_2^2\) ist gegeben. <br />
Also: \(\nabla_xf(x)=\left(\begin{array}{c} f'_{x_1} \\ f'_{x_2} \end{array}\right)= \left(\begin{array}{c} 2x_1 \\ -2x_2 \end{array}\right)\) <br />
Wir starten mit einem beliebigen Punkt: z.B. \(\left(\begin{array}{c} 2 \\ 1 \end{array}\right)\) und setzen ein: <br />
\(\left(\begin{array}{c} 2x_1 \\ -2x_2 \end{array}\right) = \left(\begin{array}{c} 2 * 2 \\ -2 * 1 \end{array}\right) = \left(\begin{array}{c} 4 \\ -2 \end{array}\right)\)
</p>

<p>
\(Neuer\ Punkt = \left(\begin{array}{c} 2 \\ 1 \end{array}\right)  - \lambda \left(\begin{array}{c} 4 \\ -2 \end{array}\right)\) mit \(\lambda: learning\ rate\)
</p>

</section>
<section id="slide-orga21a88a">
<h3 id="orga21a88a">Visualisiert</h3>

<div class="figure">
<p><img src="./gradient_descent.gif" alt="gradient_descent.gif" height="500" />
</p>
</div>

</section>
<section id="slide-orgfb8a05c">
<h3 id="orgfb8a05c">Computational Graphs</h3>
<aside class="notes">
<p>
Rechenoperationen in ANN&rsquo;s werden typischerweise nicht in mathematischen Formeln angegeben, sondern in sog. computational Graphs.
</p>

</aside>

<p>
Typischerweise werden Operationen in artificial neural networks nicht mit mathematischen Formeln angegeben, sondern als Graph dargestellt.
</p>

</section>
<section id="slide-org46c394b">
<h4 id="org46c394b">Repräsentation</h4>
<p>
Jede Node in einem Graph \(G\) repräsentiert eine mathematische Operation oder eine Input Variable.<br />
</p>

<p>
Beispielsweise:
</p>
<ul>
<li>Matrix Multiplikation</li>
<li>Addition</li>
<li>Skalare Multiplikation</li>

</ul>

</section>
<section id="slide-org17d4622">
<h4 id="org17d4622">Addition Beispiel</h4>
<p>
\[
y = a+b
\]
</p>


<div class="figure">
<p><img src="./basic_graph.png" alt="basic_graph.png" />
</p>
</div>

</section>
<section id="slide-orgd6ba594">
<h4 id="orgd6ba594">Komplexere Beispiele</h4>
<p>
\(x=y+z\) <br />
\(a=x\odot z\)
</p>


<div class="figure">
<p><img src="./basic_graph_2.jpg" alt="basic_graph_2.jpg" />
</p>
</div>

</section>
</section>
<section>
<section id="slide-org04291d2">
<h2 id="org04291d2">Künstliche Neuronale Netze &amp; Deep Learning</h2>
<div class="outline-text-2" id="text-org04291d2">
</div>
</section>
<section id="slide-org7a4487a">
<h3 id="org7a4487a">Künstliche Neuronale Netze</h3>
<p>
Als Vorbild dienen Neuronale Netze in der Biologie, jedoch sind beide Felder doch unterschiedlicher als man vielleicht erwarten würde.
</p>

<blockquote>
<p>
In diesem Vortrag werden nur fully connected feed forward networks behandelt.
</p>
</blockquote>

</section>
<section id="slide-org28eca0c">
<h3 id="org28eca0c">Formale Definition</h3>
<blockquote>
<p>
Parameter werden üblicherweise als Theta (\(\theta\)) notiert. <br />
Der Lernalgorithmus soll die Parameter \(\theta\) so verändern, dass sich \(f\) so nah wie möglich an \(f^*\) annähert.
</p>
</blockquote>

<p>
Formale Definition für ein neuronales Netz: \(y=f(x; \theta)\) und \(y = f^*(x)\)
</p>
<ul>
<li>\(y\) ist den Wert den unser NN voraussagen soll</li>
<li>\(x\) sind die Input Daten, die das NN erhält</li>
<li>\(\theta\) sind alle Parameter eines neuronalen Netzwerks</li>
<li>\(f^*\) ist unsere Zielfunktion</li>

</ul>

</section>
<section id="slide-orge5069fd">
<h3 id="orge5069fd">Wie ist nun ein neuronales Netzwerk aufgebaut?</h3>
<aside class="notes">
<p>
In der gezeigten Grafik bilden alle vertikal angeordneten Neuronen einen Layer in einem NN ab.
</p>

</aside>

<p>
Wir teilen das Netzwerk in Schichten (Layer) auf.
</p>


<div class="figure">
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3d/Neural_network.svg/1200px-Neural_network.svg.png" alt="1200px-Neural_network.svg.png" width="50%" height="50%" />
</p>
</div>

<p>
Jeder Layer bildet eine Funktion \(f^{i}\), mit \(i=Layer\ Index\) ab.
</p>

</section>
<section id="slide-orgfdf9fa3">
<h4 id="orgfdf9fa3">Formell</h4>
<p>
Somit ist ein neurales Netzwerk eine Kette an Funktionen \(f\).
</p>
<blockquote>
<p>
Ein Netz mit \(3\) Layern wäre somit \(f^2(f^1(f^0(X)))\)
mit \(X=Input\ Data\)
</p>
</blockquote>

</section>
<section id="slide-org4102b90">
<h4 id="org4102b90">Aufbau eines Layers</h4>
<p>
Jeder Layer enthält <b>mindestens</b> folgende Informationen:
</p>
<ul>
<li>Eine Weight Matrix (\(w\))</li>
<li>Einen Bias Vektor (\(b\))</li>
<li>Aktivierungsfunktion (\(\sigma\))</li>

</ul>

</section>
<section id="slide-org609871e">
<h4 id="org609871e">Verbindung der Layer</h4>
<aside class="notes">
<p>
Jede Kante in einem NN Modell repräsentiert eine reelle Zahl. <br />
Da jedes Neuron mit allen Neuronen des vorherigen Layers verbunden ist, besitzt
jedes Neuron einen Vektor mit der Größe des vorherigen Layers.
Also besitzt jeder Layer eine Weight Matrix mit den Dimensionen \(size(L_{i-1}) \times size(L_i)\)
</p>

</aside>
<p>
Jedes Neuron eines Layers \(L_i\) ist mit allen Neuronen des Layers \(L_{i-1}\) verbunden. <br />
</p>


<div class="figure">
<p><img src="./connections.jpg" alt="connections.jpg" />
</p>
</div>

</section>
<section id="slide-org3aa6e28">
<h4 id="org3aa6e28">Aktivierungsfunktion</h4>
<aside class="notes">
<p>
Non linearity Functions werden benötigt, da eine zwei lineare Funktionen
immer zu einer weiteren linearen Funktion reduziert werden können. <br />
<b>Ausnahme</b> bei einem Output Layer dessen Output eine reelle Zahl sein soll.
</p>

</aside>

<p>
Da wir bei Neural Networks oft versuchen non-lineare Zusammenhänge zu approximieren, benötigen wir auch eine nicht-lineare Komponente in unserem NN.
</p>

</section>
<section id="slide-org8274157">
<h4 id="org8274157">Beliebte Aktivierungsfunktionen</h4>
<ul>
<li><p>
Rectified Linear Unit (\(ReLU\)) <br />
</p>
<blockquote>
<p>
\(f(x)=max(0, x)\)
</p>
</blockquote></li>
<li><p>
\(Leaky\ ReLU\) <br />
</p>
<blockquote>
<p>
\(f(x)=\begin{cases} x &\quad if\ x > 0 \\ 0.1x &\quad else \end{cases}\)
</p>
</blockquote></li>
<li><p>
Sigmoid Function <br />
</p>
<blockquote>
<p>
\(f(x)= \frac{1}{1+e^{-t}}\)
</p>
</blockquote></li>

</ul>

</section>
<section id="slide-org106c11e">
<h4 id="org106c11e">Function Plot</h4>

<div class="figure">
<p><img src="./activation_functions.jpg" alt="activation_functions.jpg" />
</p>
</div>

</section>
<section id="slide-org63c631d">
<h4 id="org63c631d">Cost Function (\(J\))</h4>
<p>
Eine Funktion um zu bestimmen wie &rsquo;nah&rsquo; wir uns an unserem erwarteten Inference Wert befinden.
</p>
<blockquote>
<p>
In dieser Präsentation benutzen wir die Euklidean-Distance \((x-y)^2\) als Cost Function.
</p>
</blockquote>

</section>
</section>
<section>
<section id="slide-org4fa42c2">
<h2 id="org4fa42c2">Forward Propagation</h2>
<p>
Ein Layer in einem Feed-Forward Neural Network besteht aus folgenden Elementen:
</p>
<ul>
<li>Inputs (\(X\))</li>
<li>Weights (\(W\))</li>
<li>Biases (\(b\))</li>
<li>Output (\(a\))</li>

</ul>
</section>
<section id="slide-orgb31a2c1">
<h3 id="orgb31a2c1">Berechnung des Inputs</h3>
<p>
Jedes Neuron enthält einen Vektor mit Weights \(w\), der Angibt wie stark jeder Input gewichtet wird. <br />
\(z=a_1*w_1+a_2*w_2\) oder \(z=Wa_{L-1}\)
</p>


<div class="figure">
<p><img src="./connections.jpg" alt="connections.jpg" />
</p>
</div>

</section>
<section id="slide-org6884cdc">
<h3 id="org6884cdc">Formell</h3>
<p>
Um die Aktivierungen (\(a\)) eines Layers zu berechen können wir folgende Formel benutzen:
</p>

<blockquote>
<p>
\(a_L = \sigma(W_La_{L-1} + b_L)\)
</p>
</blockquote>
<p>
Der berechnete Vektor \(a_L\) dient dem Layer \(L+1\) als Input.
</p>

</section>
<section id="slide-orgb00a22f">
<h3 id="orgb00a22f">Computational Graph</h3>
<p>
\[
a = \sigma(W_La_{L-1}+b)
\]
</p>


<div class="figure">
<p><img src="./forward_prop_graph.png" alt="forward_prop_graph.png" />
</p>
</div>

</section>
<section id="slide-orgc3dc462">
<h3 id="orgc3dc462">Beispiel (XOR)</h3>
<p>
\(W=\left[\begin{array}{ccc} 1 & 1 \\ 1 & 1 \end{array}\right]\) <br />
\(b=\left [\begin{array}{ccc} 0 \\ -1 \end{array} \right]\) <br />
</p>
</section>
<section id="slide-org13577fd">
<h3 id="org13577fd">Multiplizieren der Weights (\(W\)) und Inputs (\(X\))</h3>
<p>
\[
XW=\left[\begin{array}{ccc} 0 & 0 \\ 0 & 1 \\ 1 & 0 \\ 1 & 1 \end{array} \right]
\left[\begin{array}{ccc} 1 & 1 \\ 1 & 1 \end{array}\right]=
\left[\begin{array}{ccc} 0 & 0 \\ 1 & 1 \\ 1 & 1 \\ 2 & 2 \end{array} \right]
\]
</p>

</section>
<section id="slide-orgdc1f8d6">
<h3 id="orgdc1f8d6">Addieren des Bias Vektors (\(b\))</h3>
<p>
\[
XW + b=
\left[\begin{array}{ccc} 0 & 0 \\ 1 & 1 \\ 1 & 1 \\ 2 & 2 \end{array} \right] +
\left(\begin{array}{ccc} 0 \\ -1 \end{array}\right)=
\left[\begin{array}{ccc} 0 & -1 \\ 1 & 0 \\ 1 & 0 \\ 2 & 1 \end{array} \right]
\]
</p>
</section>
<section id="slide-orgf936a02">
<h3 id="orgf936a02">Aktivierungsfunktion (in diesem Fall \(ReLU\))</h3>
<blockquote>
<p>
\(ReLU:= f(x)=max(0, x)\)
</p>
</blockquote>
<p>
\[
relu(XW+b)=
relu(\left[\begin{array}{ccc} 0 & -1 \\ 1 & 0 \\ 1 & 0 \\ 2 & 1 \end{array} \right])=
\left[\begin{array}{ccc} 0 & 0 \\ 1 & 0 \\ 1 & 0 \\ 2 & 1 \end{array} \right]
\]
</p>

<p>
Die Aktivierungsfunktion wird auf jedes Element der Matrix ausgeführt.
</p>

</section>
<section id="slide-org68f630c">
<h3 id="org68f630c">Output Layer</h3>
<p>
Multiplizieren der Output Matrix des ersten Layers mit den Weights des Output Layers (\(w\)).
\[
w= relu(XW+b)* \left[\begin{array}{ccc} 1 \\ -2 \end{array}\right]=
\left[\begin{array}{ccc} 0 & 0 \\ 1 & 0 \\ 1 & 0 \\ 2 & 1 \end{array} \right]*
\left[\begin{array}{ccc} 1 \\ -2 \end{array}\right]=
\left[\begin{array}{ccc} 0 \\ 1 \\ 1 \\ 0 \end{array}\right]
\]
</p>

</section>
<section id="slide-org3c1c7b9">
<h3 id="org3c1c7b9">Predictions &amp; Input</h3>
<p>
Input: \(\left[\begin{array}{ccc} 0 & 0 \\ 0 & 1 \\ 1 & 0 \\ 1 & 1 \end{array} \right]\) <br />
Predictions: \(\left[\begin{array}{ccc} 0 \\ 1 \\ 1 \\ 0 \end{array}\right]\)
</p>

</section>
<section id="slide-orgd7a0196">
<h3 id="orgd7a0196">Code Beispiel</h3>
<div class="org-src-container">

<pre  class="src src-python"><span style="color: #fb2874;">def</span> <span style="color: #b6e63e;">forward</span>(X):
    <span style="color: #fd971f;">a</span> = X
    <span style="color: #fb2874;">for</span> layer <span style="color: #fb2874;">in</span> L:
        <span style="color: #fd971f;">a</span> = layer.weights @ a + layer.bias
    <span style="color: #fb2874;">return</span> a
</pre>
</div>

</section>
<section id="slide-orge03e7c0">
<h3 id="orge03e7c0">Laufzeitkomplexität</h3>
<aside class="notes">
<p>
Wir multiplizieren jedes Weight und addieren einen Bias Wert.
</p>

</aside>

<blockquote>
<p>
\[
O(w)
\]
</p>
</blockquote>

<ul>
<li>\(w\) Anzahl der Weights in neuronalem Netz.</li>

</ul>

</section>
</section>
<section>
<section id="slide-org917fee8">
<h2 id="org917fee8">Backpropagation</h2>
<div class="outline-text-2" id="text-org917fee8">
</div>
</section>
<section id="slide-orgd23adfa">
<h3 id="orgd23adfa">Wozu brauchen wir den Backpropagation Algorithmus?</h3>
<aside class="notes">
<p>
Gesuchte Gradients:
</p>
<ul>
<li>Ableitung von \(J\) in Abhängigkeit von Bias \(b^k\)</li>
<li>Ableitung von \(J\) in Abhängigkeit von Weights \(w^k\)</li>

</ul>

</aside>

<p>
Ein fundamentaler Baustein, von neuralen Netzen.
</p>

<p>
Backpropagation ist kein Lernalgorithmus/Optimierungsalgorithmus, sondern aussschlißlich für die Generierung der Gradients jedes Layers zuständig.
</p>

<p>
Also suchen wir folgende Gradients:
</p>
<ul>
<li>\(\nabla_{b^k} J\)</li>
<li>\(\nabla_{w^k} J\)</li>

</ul>

</section>
<section id="slide-org8f354e2">
<h3 id="org8f354e2">Kettenregel</h3>
<aside class="notes">
<p>
Da ein NN prinzipiell nur viele geschachtelte Funktionen sind ist die Kettenregel sehr nützlich um die Ableitungen für jede Funktion zu bestimmen.
</p>

</aside>

<p>
Die Kettenregel ist nützlich um Ableitungen aus schon bereits vorhandenen Ableitungen zu konstruieren.
</p>

<p>
\[y=g(x)\ und\ z=f(g(x))=f(y)\]
</p>

<p>
Dann besagt die Kettenregel: \(\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}\)
</p>

</section>
<section id="slide-orga529320">
<h3 id="orga529320">Kettenregel als Graph</h3>
<aside class="notes">
<p>
An der Formel \(f'(f(f(w)))f'(f(w))f'(w)\) erkennt man, dass immer die Zwischenergebnisse aus jedem Schritt benötigt werden um die korrekte Ableitung \(\frac{\partial z}{\partial w}\) zu bestimmen.
</p>

</aside>

<p>
\[
x = f(w),\ y=f(x),\ z=f(y)
\]
</p>


<div class="figure">
<p><img src="./chain_rule_derriv.jpg" alt="chain_rule_derriv.jpg" />
</p>
</div>

<p>
\[
\frac{\partial z}{\partial w}=
\frac{\partial z}{\partial y}
\frac{\partial y}{\partial x}
\frac{\partial x}{\partial w}
=
f'(y)f'(x)f'(w) \\
= f'(f(f(w)))f'(f(w))f'(w)
\]
</p>

</section>
<section id="slide-org7a944bd">
<h3 id="org7a944bd">Anpassung der Forward Propagation</h3>
<aside class="notes">
<p>
Wie davor gezeigt müssen wir nun Zwischenergebnisse aus der Forward Progagation speichern um im Anschluss effizient die Backpropagation durchführen zu können.
Eine Alternative ist bei <b>limitiertem Speicher</b> die Zwischenergebnisse immer neu zu evaluieren, wenn sie benötigt werden. (-&gt; Höhere Laufzeit)
</p>

</aside>

<p>
Wir benötigen folgende Werte aus jedem Layer um den Backpropagation Algorithmus ausführen zu können.
</p>
<ul>
<li>\(a\) Aktivation Vektor</li>
<li>\(z\) Pre Activation Function Vektor</li>

</ul>

<blockquote>
<p>
\(f'(y)f'(x)f'(w)\): Speichern der Zwischenergebnisse in Variablen
\(f'(f(f(w)))f'(f(w))f'(w)\): Neu Evaluierung der Zwischenergebnisse
</p>
</blockquote>
</section>
<section id="slide-org8534606">
<h3 id="org8534606">Beschreibung des Algorithmus</h3>
<div class="outline-text-3" id="text-org8534606">
</div>
</section>
<section id="slide-org9f09569">
<h4 id="org9f09569">Schritt 1</h4>
<p>
Forward Propagation ausführen.
</p>
</section>
<section id="slide-orgc7c567b">
<h4 id="orgc7c567b">Schritt 2</h4>
<p>
Wir berechnen den Gradienten der Cost Function \(J\).
\(J = \frac{1}{2} (y-X)^2 \rightarrow \nabla_y J = X - y\)
</p>
</section>
<section id="slide-org477939f">
<h4 id="org477939f">Schritt 3</h4>
<p>
Erst müssen wir den Gradienten in Relation zu den pre activation function values berechnen.
</p>
<blockquote>
<p>
\(\nabla_{a^{k}} J = g \odot f'(a^{(k)})\)
</p>
</blockquote>
<p>
mit \(f'(x) := Ableitung\ der\ Aktivierungsfunktioin\)
</p>
</section>
<section id="slide-orgd002144">
<h4 id="orgd002144">Schritt 4</h4>
<p>
Weight Gradienten berechnen.
\[
f(w, a, b) = w*a+b
\]
\(g * \frac{\partial}{\partial w} = g * (a+0)\)
</p>

<blockquote>
<p>
\(\nabla_{w^k} J = ga^{k-1}\)
</p>
</blockquote>

</section>
<section id="slide-orgba697ba">
<h4 id="orgba697ba">Schritt 5</h4>
<p>
Bias Gradienten berechnen.
\[
f(w, a, b) = w*a+b
\]
\(g * \frac{\partial}{\partial b}= g * 1\)
</p>

<blockquote>
<p>
\(\nabla_{b^{k}} J = g\)
</p>
</blockquote>

</section>
<section id="slide-orgc7be62c">
<h4 id="orgc7be62c">Schritt 6</h4>
<p>
\(\nabla a^{k-1} J = w^kg\)
</p>
</section>
<section id="slide-org9f94945">
<h4 id="org9f94945">Wiederholen von Schritt 3 - 5 des nächsten Layers (\(L{-1}\))</h4>
</section>
<section id="slide-orgae8e10e">
<h3 id="orgae8e10e">Graph</h3>
<aside class="notes">
<p>
Wir bilden einen Pfad (von Hinten nach Vorne) an Pfeilen zu einem Gradienten einer Node die wir berechnen wollen.
Wir multiplizieren alle partiellen Ableitungen auf dem Weg dahin miteinander.
</p>

</aside>


<div class="figure">
<p><img src="./backprop_derriv.jpg" alt="backprop_derriv.jpg" />
</p>
</div>

</section>
<section id="slide-org62f8c36">
<h3 id="org62f8c36">Delta Rule</h3>
<p>
In neural Networks kann der Backpropagation Algorithmus zu der sog. <b>Delta Rule</b> zusammengefasst werden.
</p>
<aside class="notes">
<p>
\(\lambda\) ist learning rate <br />
\(\alpha\) ist die Aktivierungsfunktion <br />
\(z\) ist inputs * weights <br />
</p>

</aside>

<blockquote>
<p>
\[
\nabla w_{ji} = \lambda ( - a) \alpha'(z)a_{L-1}
\]
</p>
</blockquote>

</section>
<section id="slide-org25bed57">
<h3 id="org25bed57">Praktisches Beispiel in Python</h3>
<p>
<a href="https://github.com/SirBubbls/backpropagation-seminar/blob/master/Backpropagation.ipynb">Notebook</a>
</p>
<div class = "stretch">
     <iframe width="100%" height="100%" src="http://localhost:8888/lab"></iframe>
</div>
</section>
<section id="slide-org92285cb">
<h4 id="org92285cb">Mini Batch Training</h4>
<aside class="notes">
<p>
Keine Vektoren sondern mehrere Datenpunkte in Form einer Matrix (ein Vektor aus Vektoren (Inputs)).
</p>

</aside>

<p>
In der Praxis werden keine Vektoren als Input Daten benutzt, sondern Matrizen (siehe <code>XOR</code> Beispiel).
\[
Input = \left[\begin{array}{ccc} 0 & 0 \\ 0 & 1 \\ 1 & 0 \\ 1 & 1 \end{array} \right]
\]
</p>

<p>
Wir erhalten nun auch mehrere Gradienten in Form einer Matrix. Wir können nun den Durchschnitt der Gradienten nutzen um unsere Weights anzupassen.
</p>
</section>
<section id="slide-org267e2af">
<h4 id="org267e2af">Iris Dataset</h4>
<p>
<a href="https://github.com/SirBubbls/backpropagation-seminar/blob/master/MiniBatch.ipynb">Notebook</a>
</p>

<div class="figure">
<p><img src="./dataset.jpg" alt="dataset.jpg" height="500" />
</p>
</div>
</section>
<section id="slide-orgc5ba6fe">
<h4 id="orgc5ba6fe">Low Learning Rate</h4>

<div class="figure">
<p><img src="./low_learning_rate.gif" alt="low_learning_rate.gif" height="550" />
</p>
</div>

</section>
<section id="slide-org73b0a40">
<h4 id="org73b0a40">High Learning Rate</h4>

<div class="figure">
<p><img src="./high_learning_rate.gif" alt="high_learning_rate.gif" height="550" />
</p>
</div>

</section>
<section id="slide-org9f995a3">
<h3 id="org9f995a3">Komplexität</h3>
<blockquote>
<p>
<b>Wichtig</b> <br />
Folgende Komplexitäten beziehen sich ausschließlich auf den Backpropagation Algorithmus.
</p>
</blockquote>

</section>
<section id="slide-org5a96a7d">
<h4 id="org5a96a7d">Laufzeitkomplexität</h4>
<aside class="notes">
<p>
Wir multiplizieren die transponierte Weight Matrix also die gleiche Komplexität wie
Forward-propagation.
</p>

</aside>

<p>
Backpropagation besitzt die gleiche Laufzeitkomplexität wie Forward-propagation.
</p>

<blockquote>
<p>
\[
O(w)
\]
</p>
</blockquote>

<ul>
<li>\(w\) Anzahl der Weights in neuronalem Netz.</li>

</ul>

</section>
<section id="slide-org8237860">
<h4 id="org8237860">Speicherkomplexität</h4>
<blockquote>
<p>
\[
O(mh)
\]
</p>
</blockquote>

<ul>
<li>\(m\) Anzahl an Elementen in Batch</li>
<li>\(h\) Anzahl der Hidden-Units</li>

</ul>


</section>
</section>
<section>
<section id="slide-org8b33e70">
<h2 id="org8b33e70">General Backpropagation</h2>
<p>
Bisher haben wir uns nur mit Backpropagation in Zusammenhang mit neuronalen Netzwerken beschäftigt. <br />
Backpropagation kann aber auch generell für andere Anwendungen eingesetzt werden.
</p>

</section>
<section id="slide-orgcc9164e">
<h3 id="orgcc9164e">Symbol to Number / Symbol to Symbol</h3>
<p>
Es existieren zwei verschiedene Möglichkeiten die Berechnungen der Gradients durchzuführen.
</p>

<ul>
<li>Symbol to Number</li>
<li>Symbol to Symbol</li>

</ul>

</section>
<section id="slide-org395c622">
<h4 id="org395c622">Symbol to Number</h4>
<aside class="notes">
<p>
Methode die wir in vorherigen Beispielen verwendet waren.
</p>

</aside>

<p>
Die Input Variablen werden durch Zahlenwerte ersetzt und daraufhin (wie besprochen) alle nötigen Gradienten berechnet.
</p>

</section>
<section id="slide-org6699cc9">
<h4 id="org6699cc9">Symbol to Symbol</h4>
<aside class="notes">
<p>
Symbol to Symbol benötigt zum differenzieren keine eigentlichen Zahlenwerte, sondern ersetzt diese durch Symbole. <br />
Zusammengefasst kann man sagen, dass der Symbol to Number approach nur die Berechnungen ausführt die vom Symbol to Symbol als Graph erstellt werden.
</p>

</aside>

<p>
Beim der Symbol to Symbol Herangehensweise wird zuerst der Graph mit allen Ableitungen mit der Hilfe von symbolischen Werten konstruiert. <br />
Später wird dann der Graph mit der Hilfe eines eigenen Algorithmus ausgewertet. <br />
</p>

<blockquote>
<p>
<b>Vorteil</b> <br />
Ableitungen eines höheren Grads können berechnet werden, indem man den Backpropagation Algorithmus auf einen bereits abgeleiteten Graphen ausführt.
</p>
</blockquote>

</section>
<section id="slide-org76cc3f9">
<h3 id="org76cc3f9">Operationen</h3>
<aside class="notes">
<p>
Wir benutzen Tensoren um eine möglichst generelle Definition des Algorithmus zu beschreiben.
</p>

</aside>
<p>
Wir betrachten einen computational Graph, jede Node in dem Graph repräsentiert eine Variable in Form eines Tensors.
</p>

</section>
<section id="slide-org1bd7dd7">
<h4 id="org1bd7dd7">Funktionen</h4>
<aside class="notes">
<p>
<code>get_operation</code> Beispiel bei einer Variable, die durch Matrix Multiplikation generiert wird, würde genau diese Operation zurück gegeben werden.
</p>

</aside>

<ul>
<li><code>get_operation()</code></li>
<li><code>get_consumers()</code> <br />
Gibt alle Variablen/Operationen zurück, die &rsquo;Kinder&rsquo; von sich selber sind.</li>
<li><code>get_inputs()</code> <br />
Gibt alle Variablen/Operationen zurück, die &rsquo;Eltern&rsquo; von sich selber sind.</li>
<li><code>bprop()</code> <br />
Muss bei jeder Operation implementiert werden.</li>

</ul>
</section>
<section id="slide-org8269d04">
<h3 id="org8269d04">Algorithmus</h3>
<p>
Benötigt ist:
</p>
<ul>
<li>die Menge aller Variablen \(T\), deren Gradienten wir berechnen müssen</li>
<li>den Graphen \(G\)</li>
<li>die Variable \(z\), die wir differenzieren wollen</li>

</ul>

</section>
<section id="slide-org8299ef1">
<h4 id="org8299ef1">Äußere Funktion</h4>
<p>
Wir definieren \(G'\) als alle Variablen, die Vorfahren von \(z\) und Nachfahren von \(T\) sind. <br />
<br />
In <code>grad_table</code> können wir Variablen Gradients zuweisen. <br />
</p>

<p>
<code>grad_table[z] = 1</code>   (da \(\frac{\partial z}{\partial z} = 1\))
</p>

</section>
<section id="slide-org56b8760">
<h4 id="org56b8760">Loop über alle Variablen, deren Gradienten wir berechnen müssen</h4>
<p>
In jedem Loop rufen wir die Funktion <code>build_grad</code> auf.
</p>
<div class="org-src-container">

<pre  class="src src-python"><span style="color: #fb2874;">for</span> v <span style="color: #fb2874;">in</span> T:
    build_grad(v, G, G_1, grad_table)
<span style="color: #fb2874;">return</span> [grad_table[v] <span style="color: #fb2874;">for</span> v <span style="color: #fb2874;">in</span> T]
</pre>
</div>

</section>
<section id="slide-org2e81c82">
<h4 id="org2e81c82"><code>build_grad(v, G, G_1, grad_table)</code></h4>
<aside class="notes">
<p>
Es handelt sich um eine <b>rekursive</b> Funktion.<br />
Der Base Case ist erreicht, sobald sich der zu berechnende Gradient sich bereits in <code>grad_table</code> befindet.<br />
<br />
Um den Gradient zu berechnen benötigen wir erst alle Ableitungen der Consumer aus \(G'\). <br />
<br />
Alle Gradients der Consumer werden summiert und daraufhin der Node zugeordnet.
</p>

</aside>

<div class="org-src-container">

<pre  class="src src-python"><span style="color: #fb2874;">def</span> <span style="color: #b6e63e;">build_grad</span>(v, G, G_1, grad_table):
    <span style="color: #fb2874;">if</span> v <span style="color: #fb2874;">in</span> grad_table: <span style="color: #fb2874;">return</span> grad_table[v]
    <span style="color: #fd971f;">i</span> = <span style="color: #9c91e4; font-weight: bold;">1</span>
    <span style="color: #fb2874;">for</span> c <span style="color: #fb2874;">in</span> get_consumers(v, G_1):
        <span style="color: #fd971f;">op</span> = get_operation(c)
        <span style="color: #fd971f;">d</span> = build_grad(c, G, G_1, grad_table)
        <span style="color: #fd971f;">g</span>[i] = op.bprop(get_inputs(c, G_1), v, d)
        <span style="color: #fd971f;">i</span> += <span style="color: #9c91e4; font-weight: bold;">1</span>
    <span style="color: #fd971f;">g</span> = <span style="color: #fd971f;">sum</span>(g)
    <span style="color: #fd971f;">grad_table</span>[v] = g
    <span style="color: #fb2874;">return</span> g
</pre>
</div>

</section>
<section id="slide-org3967b58">
<h4 id="org3967b58"><code>bprop</code> Funktion</h4>
<aside class="notes">
<p>
Auch diese Funktion kann natürlich auch in einer anderen Sprache implementiert sein.
</p>

</aside>

<p>
<code>op.bprop(inputs, X, G)</code> <br />
 <br />
<code>inputs</code>: Liste an Inputs, die wir der Operation zur Verfügung stellen <br />
<code>X</code>: Input, dessen Ableitung wir berechnen wollen <br />
<code>G</code>: Gradient des Outputs der Operation
</p>

</section>
<section id="slide-orgbcf1f21">
<h3 id="orgbcf1f21">Beispiel</h3>
<div class="outline-text-3" id="text-orgbcf1f21">
</div>
</section>
<section id="slide-org3fdf152">
<h4 id="org3fdf152">Graph</h4>
<p>
Wir wollen \(\frac{\partial u_1}{\partial u_4}\) bestimmen.
<img src="./big_graph_1.jpg" alt="big_graph_1.jpg" />
</p>

</section>
<section id="slide-orga656207">
<h4 id="orga656207">Bestimmen der Ableitung \(\frac{\partial u_1}{\partial u_4}\)</h4>
<aside class="notes">
<p>
Wir müssen erst \(G'\) bestimmen, also:
</p>
<ul>
<li>Vorfahren von \(u_1 = z\)</li>
<li>Nachfahren von \([u_4] = T\)</li>

</ul>

</aside>


<div class="figure">
<p><img src="./big_graph_2.jpg" alt="big_graph_2.jpg" />
</p>
</div>

</section>
<section id="slide-org585e29e">
<h4 id="org585e29e">Eintragen aller Ableitungen in Graph</h4>
<aside class="notes">
<p>
Wir tragen alle Operationen, die zur Berechnung der gesuchten Ableitung benötigt werden in den Graphen \(G\) ein.
</p>

</aside>


<div class="figure">
<p><img src="./big_graph_3.jpg" alt="big_graph_3.jpg" height="550" />
</p>
</div>

</section>
<section id="slide-org694a1f8">
<h3 id="org694a1f8">Generalisierbarkeit</h3>
<p>
Dadurch ist der Backpropagation Algorithmus sehr allgemein anwendbar. <br />
</p>

<p>
Jede Operation ist für seine eigene Differenzierung verantwortlich und benötigt keine weiteren Informationen.
</p>

</section>
</section>
<section>
<section id="slide-org2594bdf">
<h2 id="org2594bdf">Historisches</h2>
<aside class="notes">
<p>
Die Kettenregel stammt aus dem 17ten Jahrhundert.
</p>

</aside>

<ul>
<li>Kettenregel stammt aus dem 17ten Jahrhundert (Leibniz, 1676). <br /></li>
<li>Lineare neurale Netzwerke Mitte des 20ten Jahrhunderts. <br /></li>
<li>Erfolgreiche Experimente mit Back-Propagation (1986) <br /></li>

</ul>

</section>
<section id="slide-org0c50908">
<h4 id="org0c50908">Popularität von neuronalen Netzen</h4>
<aside class="notes">
<p>
Durch die Erfolge mit Backpropagation wurde Anfang der 90er Jahre Deep Learning vermehrt eingesetzt. <br />
Klassische machine learning Algorithmen wurden in den 90er Jahren mehr genutzt als neuronale Netzwerke.
</p>

</aside>

<p>
Klassische machine learning Algorithmen wurden in den 90er Jahren mehr genutzt als neuronale Netzwerke.
</p>

<p>
Durch die hohe Speicheranforderung wurden NN ab ca. 2006 immer vermehrter eingesetzt und
bilden heute einen fundamentalen Baustein von maschinellem Lernen.
</p>

</section>
<section id="slide-org532c88f">
<h4 id="org532c88f">Backpropagation &amp; Gradient Descent</h4>
<p>
Beide treibenden Algorithmen von neuronalen Netzwerken haben sich seit den 80er Jahren nicht wesentlich verändert. <br />
</p>

<p>
Bessere Resultate sind besser Hardware und Dataset Optimierung zu verdanken.
</p>

</section>
</section>
<section>
<section id="slide-org0a27d92">
<h2 id="org0a27d92">Quellen</h2>
<ul>
<li>Deep Learning (Ian Goodfellow, Yoshua Bengio &amp; Aaron Courville)</li>
<li><a href="https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c">https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c</a></li>
<li>Wikipedia: <a href="https://en.wikipedia.org/wiki/Backpropagation">https://en.wikipedia.org/wiki/Backpropagation</a></li>
<li>Wikipedia: <a href="https://en.wikipedia.org/wiki/Delta_rule">https://en.wikipedia.org/wiki/Delta_rule</a></li>

</ul>
</section>
</section>
</div>
</div>
<script src="reveal/lib/js/head.min.js"></script>
<script src="reveal/js/reveal.js"></script>
<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c',
rollingLinks: false,
keyboard: true,
mouseWheel: false,
fragmentInURL: false,
hashOneBasedIndex: false,
pdfSeparateFragments: true,

overview: true,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'convex', // see README of reveal.js for options
transitionSpeed: 'default',

// Optional libraries used to extend reveal.js
dependencies: [
 { src: 'reveal/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'reveal/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'reveal/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'reveal/plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'reveal/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } }]

});
</script>
</body>
</html>
