<!DOCTYPE html>
<html lang="de">
<head>
<meta charset="utf-8"/>
<title>Backpropagation</title>
<meta name="author" content="Lucas Sas Brunschier"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js/css/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js/css/theme/black.css" id="theme"/>

<link rel="stylesheet" href="style.css"/>

<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdn.jsdelivr.net/npm/reveal.js/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<h1 class="title">Backpropagation</h1><h2 class="author">Lucas Sas Brunschier</h2><h2 class="date">SS20</h2><p class="date">Created: 2020-04-19 Sun 00:40</p>
</section>

<section>
<section id="slide-org4d117d1">
<h2 id="org4d117d1">Intro</h2>
<p>
Präsentation ist auch online <a href="https://sirbubbls.github.io/backpropagation-seminar">sirbubbls.github.io/backpropagation-seminar</a>
</p>

<p>
Präsentation ist in Org-Mode geschrieben, also sourcen aller Grafiken und
Beispiele sind integriert.
</p>
</section>
<section id="slide-org0125aab">
<h3 id="org0125aab">Zusätzliche Ressourcen</h3>
<p>
Deep Learning (Ian Goodfellow, Yoshua Bengio &amp; Aaron Courville)
</p>

<div class="figure">
<p><img src="https://images-eu.ssl-images-amazon.com/images/I/610HnULa0dL._SY445_QL70_ML2_.jpg" alt="610HnULa0dL._SY445_QL70_ML2_.jpg" width="150" />
</p>
</div>

<p>
<a href="https://www.deeplearningbook.org">https://www.deeplearningbook.org</a>
</p>

<p>
<a href="https://www.deeplearningbook.org/contents/mlp.html">Backpropagation Kapitel</a>
</p>

</section>
<section id="slide-org91248b8">
<h3 id="org91248b8">Notation</h3>
<div class="org-src-container">

<pre  class="src src-python"><span style="color: #e45649;">for</span> i <span style="color: #e45649;">in</span> <span style="color: #a626a4;">range</span>(<span style="color: #da8548; font-weight: bold;">5</span>):
    <span style="color: #e45649;">print</span>(i)
</pre>
</div>

<pre class="example">
0
1
2
3
4
</pre>

</section>
</section>
<section>
<section id="slide-org8ea80f1">
<h2 id="org8ea80f1">Vorwissen</h2>
</section>
<section id="slide-orged3b33e">
<h3 id="orged3b33e">Gradient Der Gradient ist der Vektor aller partiellen Ableitungen einer Funktion \(f\).</h3>
<blockquote>
<p>
Notation: \(\nabla_xf(x)\)
</p>
</blockquote>
</section>
<section id="slide-orgc434f01">
<h4 id="orgc434f01">Beispiel</h4>
<blockquote>
<p>
\(f(x, y) = 2x^2 + y^3\)
</p>
</blockquote>
<p>
\(\rightarrow \nabla_xf(x)=(\begin{array}{c} f'_x \\ f'_y \end{array})= (\begin{array}{c} 4x \\ 3y \end{array})\)
</p>

</section>
<section id="slide-orgd0810c6">
<h3 id="orgd0810c6">Stochastic Gradient Descent</h3>
<p>
Der Gradient Descent Algorithmus wird dafür verwendet ein lokales Minimum einer
Funktion zu bestimmen.
</p>
</section>
<section id="slide-org6f3360c">
<h4 id="org6f3360c">Beispiel</h4>
<p>
Funktion \(f(x)=x^2-y^2\) ist gegeben.
</p>

<div class="org-src-container">

<pre  class="src src-jupyter-python"><span style="color: #e45649;">import</span> matplotlib.pyplot <span style="color: #e45649;">as</span> plt
<span style="color: #e45649;">import</span> numpy <span style="color: #e45649;">as</span> np
<span style="color: #e45649;">import</span> sympy <span style="color: #e45649;">as</span> sp
<span style="color: #e45649;">from</span> sympy <span style="color: #e45649;">import</span> latex
sp.init_printing()

<span style="color: #6a1868;">x</span>, <span style="color: #6a1868;">y</span> = sp.symbols(<span style="color: #50a14f;">'x, y'</span>, real=<span style="color: #b751b6;">True</span>)
<span style="color: #6a1868;">F</span> = sp.Matrix([x**<span style="color: #da8548; font-weight: bold;">2</span>-y**<span style="color: #da8548; font-weight: bold;">2</span>])
<span style="color: #6a1868;">J</span> = F.jacobian(sp.Matrix([x, y]))
<span style="color: #e45649;">print</span>(latex(J))
</pre>
</div>

: \left[\begin{matrix}2 x & - 2 y\end{matrix}\right]


<ul class="org-ul">
<li><a id="orgffc0f03"></a>test<br />
<p>
\left[\begin{matrix}2 x &amp; - 2 y\end{matrix}\right]
</p>
</li>
</ul>

</section>
</section>
<section>
<section id="slide-org4cc9773">
<h2 id="org4cc9773">Neuronale Netze</h2>
<p>
Formale Definition für ein Neuronales Netz: \(y=f(x; \theta)\) und \(y=f^*(x)\)
</p>
<ul>
<li>\(y\) ist den Wert den unser NN vorraussagen soll</li>
<li>\(x\) sind die Input Daten, die das NN erhält</li>
<li>\(\theta\) sind Parameter des neuronalen Netzes, um \(f\) so nah wie möglich an
die optimale Funktion \(f^*\) anzunähern.</li>

</ul>
</section>
<section id="slide-org0d3fd98">
<h3 id="org0d3fd98">Wie ist nun ein neurales Netzwerk aufgebaut?</h3>
<p>
Wir teilen das Netzwerk in Schichten (Layer) auf.
</p>


<div class="figure">
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3d/Neural_network.svg/1200px-Neural_network.svg.png" alt="1200px-Neural_network.svg.png" width="50%" height="50%" />
</p>
</div>

<p>
Jeder Layer bildet eine Funktion \(f^{i}\), mit \(i=Layer\ Index\) ab.
</p>

</section>
<section id="slide-org6227fb1">
<h4 id="org6227fb1">Formell</h4>
<p>
Somit ist ein neurales Netzwerk eine Kette an Funktionen \(f\).
</p>

<blockquote>
<p>
Ein Netz mit \(3\) Layern wäre somit \(f^2(f^1(f^0(X)))\) mit \(X=Input\ Data\)
</p>
</blockquote>

</section>
<section id="slide-org8b36cf6">
<h4 id="org8b36cf6">Aufbau eines Layers</h4>

</section>
</section>
<section>
<section id="slide-org883dafd">
<h2 id="org883dafd">Forward Propagation</h2>
<p>
Ein Layer in einem Feed-Forward Neural Network besteht aus folgenden Elementen:
</p>
<ul>
<li>Inputs (\(X\))</li>
<li>Weights (\(W\))</li>
<li>Biases</li>
<li>Output</li>

</ul>
</section>
<section id="slide-orgf10d06f">
<h3 id="orgf10d06f">Dimensionen</h3>
</section>
<section id="slide-org0de9b2f">
<h3 id="org0de9b2f">Beispiel (XOR)</h3>
<p>
\(W=\left[\begin{array}{ccc} 1 & 1 \\ 1 & 1 \end{array}\right]\) <br />
\(c=\left [\begin{array}{ccc} 0 \\ -1 \end{array} \right]\) <br />
</p>
</section>
<section id="slide-org44c1ea4">
<h3 id="org44c1ea4">Multiplizieren der Weights (\(W\)) und Inputs (\(X\))</h3>
<p>
\[
XW=\left[\begin{array}{ccc} 0 & 0 \\ 0 & 1 \\ 1 & 0 \\ 1 & 1 \end{array} \right]
\left[\begin{array}{ccc} 1 & 1 \\ 1 & 1 \end{array}\right]=
\left[\begin{array}{ccc} 0 & 0 \\ 1 & 1 \\ 1 & 1 \\ 2 & 2 \end{array} \right]
\]
</p>
</section>
<section id="slide-orgbd44546">
<h3 id="orgbd44546">Addieren des Bias Vektors (\(c\))</h3>
<p>
\[
XW + c=
\left[\begin{array}{ccc} 0 & 0 \\ 1 & 1 \\ 1 & 1 \\ 2 & 2 \end{array} \right] +
\left(\begin{array}{ccc} 0 \\ -1 \end{array}\right)=
\left[\begin{array}{ccc} 0 & -1 \\ 1 & 0 \\ 1 & 0 \\ 2 & 1 \end{array} \right]
\]
</p>
</section>
<section id="slide-org35ed46f">
<h3 id="org35ed46f">Aktivierungsfunktion (in diesem Fall \(ReLU\))</h3>
<blockquote>
<p>
\(ReLU:= f(x)=max(0, x)\)
</p>
</blockquote>
<p>
\[
relu(XW+c)=
relu(\left[\begin{array}{ccc} 0 & -1 \\ 1 & 0 \\ 1 & 0 \\ 2 & 1 \end{array} \right])=
\left[\begin{array}{ccc} 0 & 0 \\ 1 & 0 \\ 1 & 0 \\ 2 & 1 \end{array} \right]
\]
</p>

<p>
Die Aktivierungsfunktion wird auf jedes Element der Matrix ausgeführt.
</p>

</section>
<section id="slide-org83c517b">
<h3 id="org83c517b">Output Layer</h3>
<p>
Multiplizieren der Output Matrix des ersten Layers mit den Weights des Output Layers (\(w\)).
</p>

<p>
\[
w= relu(XW+c)* \left[\begin{array}{ccc} 1 \\ -2 \end{array}\right]=
\left[\begin{array}{ccc} 0 & 0 \\ 1 & 0 \\ 1 & 0 \\ 2 & 1 \end{array} \right]*
\left[\begin{array}{ccc} 1 \\ -2 \end{array}\right]=
\left[\begin{array}{ccc} 0 \\ 1 \\ 1 \\ 0 \end{array}\right]
\]
</p>

</section>
<section id="slide-org962bc27">
<h3 id="org962bc27">Predictions &amp; Input</h3>
<p>
Input: \(\left[\begin{array}{ccc} 0 & 0 \\ 0 & 1 \\ 1 & 0 \\ 1 & 1 \end{array} \right]\)
Predictions: \(\left[\begin{array}{ccc} 0 \\ 1 \\ 1 \\ 0 \end{array}\right]\)
</p>

</section>
</section>
<section>
<section id="slide-orgb589d33">
<h2 id="orgb589d33">Backpropagation</h2>
<div class="outline-text-2" id="text-orgb589d33">
</div>
</section>
<section id="slide-orgab314e7">
<h3 id="orgab314e7">Wozu brauchen wir den Backpropagation Algorithmus?</h3>
<p>
Ein fundamentaler Baustein, von neuralen Netzen.
</p>

<p>
Backpropagation ist kein Lernalgorithmus/Optimierungsalgorithmus, sondern aussschlißlich für die Generierung der Gradients zuständig.
</p>

</section>
<section id="slide-orge9b3966">
<h3 id="orge9b3966">Chain Rule</h3>
<p>
Die Kettenregel ist nützlich um Ableitung.
</p>

<p>
\[
y=g(x)\ und\ z=f(g(x))
\]
</p>

<p>
Dann besagt die Kettenregel \(\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}\).
</p>

<p>
Vektornotation: \(\nabla_xz=\left( \frac{\partial y}{\partial x}\right) ^T \nabla_y z\)
</p>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/lib/js/head.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/js/reveal.js"></script>
<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c',
rollingLinks: false,
keyboard: true,
mouseWheel: false,
fragmentInURL: false,
hashOneBasedIndex: false,
pdfSeparateFragments: true,

overview: true,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'convex', // see README of reveal.js for options
transitionSpeed: 'default',

// Optional libraries used to extend reveal.js
dependencies: [
 { src: 'https://cdn.jsdelivr.net/npm/reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/npm/reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/npm/reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/npm/reveal.js/plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/npm/reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } }]

});
</script>
</body>
</html>
