{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "\n",
    "\n",
    "![test](https://www.i2tutorials.com/wp-content/uploads/2019/09/Deep-learning-25-i2tutorials.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def relu(z, deriv=False):\n",
    "    activations = []\n",
    "    shape = z.shape\n",
    "    z = z.flatten()\n",
    "    if deriv:  # Return Derivative of Function\n",
    "        \n",
    "        for i in range(len(z)):\n",
    "            if z[i] >= 0:\n",
    "                activations.append(1)\n",
    "            else:\n",
    "                activations.append(0.2)\n",
    "        return np.array(activations).reshape(shape)\n",
    "    \n",
    "    for i in range(len(z)):\n",
    "        if z[i] > 0:\n",
    "            activations.append(z[i])\n",
    "        else:\n",
    "            activations.append(0.2 * z[i])\n",
    "    return np.array(activations).reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight & Bias Initialization\n",
    "\n",
    "Bias Values ($b$) are initialized with $0$.  \n",
    "Weight Values ($w$) are initialized with random values between $min$ and $max$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "Weights:\n",
      " [[0.2691932  0.64935961 0.23511019]\n",
      " [0.65985629 0.86056022 0.22106011]] (Shape: (2, 3))\n",
      "Bias: \n",
      "[0. 0. 0.] (Shape: (3,))\n",
      "\n",
      "Layer 1:\n",
      "Weights:\n",
      " [[0.50063083 0.16287178 0.32707676]\n",
      " [0.59549772 0.25531792 0.95342564]\n",
      " [0.55282029 0.38249531 0.66991016]] (Shape: (3, 3))\n",
      "Bias: \n",
      "[0. 0. 0.] (Shape: (3,))\n",
      "\n",
      "Layer 2:\n",
      "Weights:\n",
      " [[0.20430429 0.81467913]\n",
      " [0.73185745 0.99687913]\n",
      " [0.95436248 0.34653977]] (Shape: (3, 2))\n",
      "Bias: \n",
      "[0. 0.] (Shape: (2,))\n",
      "\n",
      "Layer 3:\n",
      "Weights:\n",
      " [[0.76688415]\n",
      " [0.30492832]] (Shape: (2, 1))\n",
      "Bias: \n",
      "[0.] (Shape: (1,))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def initialize(min=0, max=1, do_print=True):\n",
    "    global w, b\n",
    "    w = [\n",
    "            max * np.random.uniform(min, max, (2, 3)),\n",
    "            max * np.random.uniform(min, max, (3, 3)),\n",
    "            max * np.random.uniform(min, max, (3, 2)),\n",
    "            max * np.random.uniform(min, max, (2, 1))\n",
    "        ]\n",
    "    b = [\n",
    "        np.array(np.zeros(3)),\n",
    "        np.array(np.zeros(3)),\n",
    "        np.array(np.zeros(2)),\n",
    "        np.array(np.zeros(1))\n",
    "    ]\n",
    "    if do_print:\n",
    "        for i in range(len(b)): print(f'Layer {i}:\\nWeights:\\n {w[i]} (Shape: {w[i].shape})\\nBias: \\n{b[i]} (Shape: {b[i].shape})\\n')    \n",
    "\n",
    "w, b = [], []\n",
    "\n",
    "initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "List $a$ holds each layers activation vector.  \n",
    "List $z$ holds each layers pre nonlinearity vector.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "For each layer $L$, starting with $L_0$ we multiply the $h$ vector with the weight matrix $w$.\n",
    "\n",
    "$$\n",
    "w = \\left[ \\begin{array}{rrr}\n",
    "1.3 & 0.2 \\\\                                              \n",
    "0.1 & 1.4 \\\\\n",
    "1.2 & 0 \\\\\n",
    "\\end{array}\\right] \\ \\ \\ \\ \\ \\ \\ \n",
    "h = \\left( \\begin{array}{rrr}\n",
    "1.3 \\\\                                              \n",
    "0.1 \\\\\n",
    "1.2 \\\\\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.03056778])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, z = [], []\n",
    "\n",
    "initialize(do_print=False)\n",
    "\n",
    "def forward_prop(X):\n",
    "    h = X\n",
    "    global a, z\n",
    "    a,z  = [], []\n",
    "    for i in range(len(w)):\n",
    "        h = h @ w[i] # weigt * input\n",
    "        h = h + b[i] # bias add\n",
    "        z.append(h)\n",
    "        h = relu(h) # Activation Function\n",
    "        a.append(h)\n",
    "    return h\n",
    "\n",
    "forward_prop(np.array([0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Prop\n",
    "\n",
    "for each layer\n",
    "\n",
    "$g = loss'(X,y)$\n",
    "\n",
    "## Step 1 ($a$ to $z$)\n",
    "\n",
    "$g = relu'(z)$\n",
    "\n",
    "\n",
    "\n",
    "## Step 2 ($z$ to $W$)\n",
    "\n",
    "$g = relu'(z) * a_{L-1}$\n",
    "\n",
    "\n",
    "## Step 1 Activation Function Deriv\n",
    "\n",
    "$g = [1\\times2]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.39361445]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.07746617])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "\n",
    "initialize(do_print=False)\n",
    "\n",
    "def back_prop(X, y, print_loss=False):\n",
    "    global a, z, w, b\n",
    "\n",
    "    g = (X - y).reshape(1,-1).T  # Loss Function Derivative\n",
    "    loss = 0.5*(y - X)**2\n",
    "    \n",
    "    if print_loss:\n",
    "        print(\"Loss: \", (y - X)**2)\n",
    "    \n",
    "    n_weights, n_bias = [], []\n",
    "    \n",
    "    for x in range(len(w)):\n",
    "        i = len(b) - 1 - x  # Reverse Index\n",
    "\n",
    "        # Activation Function Derivative \n",
    "        g = g * relu(z[i], True)  # Activation Function Derivative\n",
    "        \n",
    "        # Derivative with respect to weight\n",
    "        if i-1 < 0: w_der = y.reshape(1,-1).T\n",
    "        else: w_der = a[i-1].reshape(1,-1).T  # Previous Layer Activation\n",
    "        \n",
    "        # Change in Weights & Bias\n",
    "        n_weights.append(w[i] - learning_rate * (w_der @ g))\n",
    "        n_bias.append(b[i] - learning_rate * g)\n",
    "        \n",
    "        g = g @ w[i].T \n",
    "    \n",
    "    # Updating Weights\n",
    "    w = list(reversed(n_weights))\n",
    "    b = list(reversed(n_bias))\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "\n",
    "X = forward_prop(np.array([0,1]))\n",
    "print(X)\n",
    "back_prop(X, np.array([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [0.04529865]\n",
      "Loss:  [0.00946141]\n",
      "Loss:  [0.00501325]\n",
      "Loss:  [0.00289472]\n",
      "Loss:  [0.00173455]\n",
      "Loss:  [0.00089255]\n",
      "Loss:  [0.00047196]\n",
      "Loss:  [0.00036827]\n",
      "Loss:  [0.00021339]\n",
      "Loss:  [0.00013773]\n",
      "Loss:  [5.56637061e-05]\n",
      "Loss:  [3.24555633e-05]\n",
      "Loss:  [1.91940154e-05]\n",
      "Loss:  [1.2280882e-05]\n",
      "Loss:  [6.53740276e-06]\n",
      "Loss:  [3.3244694e-06]\n",
      "Loss:  [1.72690185e-06]\n",
      "Loss:  [8.77244406e-07]\n",
      "Loss:  [5.09636627e-07]\n",
      "Loss:  [2.76851063e-07]\n",
      "[[0.50005145]]\n",
      "[[0.89961736]]\n",
      "[[0.99925426]]\n",
      "[[1.99998743]]\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "def train(epochs, size=100):\n",
    "    for i in range(epochs):\n",
    "        tmp = 0.0\n",
    "        for x in range(size):\n",
    "            y1 = randint(0,1)\n",
    "            y2 = randint(0,1)\n",
    "            tmp += back_prop(forward_prop(np.array([y1,y2])), np.array([y1+y2]))[0]\n",
    "        print(\"Loss: \", tmp/size)\n",
    "\n",
    "initialize(do_print=False)\n",
    "train(20, 100)\n",
    "print(forward_prop(np.array([0.2,0.3])))\n",
    "print(forward_prop(np.array([0.7,0.2])))\n",
    "print(forward_prop(np.array([1,0])))\n",
    "print(forward_prop(np.array([1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [0.09234535]\n",
      "Loss:  [0.09735891]\n",
      "Loss:  [0.06414974]\n",
      "Loss:  [0.00248964]\n",
      "Loss:  [7.68567123e-19]\n",
      "Loss:  [8.92411225e-32]\n",
      "Loss:  [1.55449972e-32]\n",
      "Loss:  [1.54402266e-32]\n",
      "Loss:  [1.55489415e-32]\n",
      "Loss:  [1.46908087e-32]\n",
      "Loss:  [1.52341367e-32]\n",
      "Loss:  [1.50356889e-32]\n",
      "Loss:  [1.51855724e-32]\n",
      "Loss:  [1.4830092e-32]\n",
      "Loss:  [1.52366019e-32]\n",
      "Loss:  [1.47669831e-32]\n",
      "Loss:  [1.53453168e-32]\n",
      "Loss:  [1.55775377e-32]\n",
      "Loss:  [1.53275674e-32]\n",
      "Loss:  [1.50275537e-32]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[2.22044605e-16]]\n",
      "[[1.11022302e-16]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x118d7f410>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [\n",
    "    [0,1],\n",
    "    [0,0],\n",
    "    [1,0],\n",
    "    [1,1]\n",
    "]\n",
    "\n",
    "y = [1,0,1,0]\n",
    "\n",
    "initialize(-1, 1, do_print=False)\n",
    "\n",
    "def train(epochs, size=100):\n",
    "    l = []\n",
    "    for i in range(epochs):\n",
    "        tmp = 0.0\n",
    "        for x in range(size):\n",
    "            y1 = randint(0,3)\n",
    "            tmp += back_prop(forward_prop(np.array(X[y1])), np.array(y[y1]))[0]\n",
    "        l.append(tmp/size)\n",
    "        print(\"Loss: \", tmp/size)\n",
    "    return np.array(l).flatten()\n",
    "\n",
    "loss_over_time =  train(20,1000)\n",
    "\n",
    "print(forward_prop(np.array([1,0])))\n",
    "print(forward_prop(np.array([0,1])))\n",
    "print(forward_prop(np.array([0,0])))\n",
    "print(forward_prop(np.array([1,1])))\n",
    "\n",
    "plt.plot(loss_over_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Backpropagation Implementation\n",
    "Exmaple: Learning XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Loss: [0.121]\n",
      "[Epoch 1] Loss: [0.076]\n",
      "[Epoch 2] Loss: [0.065]\n",
      "[Epoch 3] Loss: [0.076]\n",
      "[Epoch 4] Loss: [0.065]\n",
      "[Epoch 5] Loss: [0.061]\n",
      "[Epoch 6] Loss: [0.071]\n",
      "[Epoch 7] Loss: [0.066]\n",
      "[Epoch 8] Loss: [0.068]\n",
      "[Epoch 9] Loss: [0.068]\n",
      "[Epoch 10] Loss: [0.068]\n",
      "[Epoch 11] Loss: [0.078]\n",
      "[Epoch 12] Loss: [0.103]\n",
      "[Epoch 13] Loss: [0.083]\n",
      "[Epoch 14] Loss: [0.01]\n",
      "[Epoch 15] Loss: [3.494e-08]\n",
      "[[ 1.000e+00]\n",
      " [ 1.000e+00]\n",
      " [-6.295e-06]\n",
      " [-3.505e-05]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "\n",
    "def backprop_entry(X, y, print_loss=False):\n",
    "    global a, z, w, b, n_weights, n_bias\n",
    "    n_weights, n_bias = [], []\n",
    "    \n",
    "    backprop_rec(0, X, y)\n",
    "    \n",
    "    # Update Weights\n",
    "    w = list(reversed(n_weights))\n",
    "    b = list(reversed(n_bias))\n",
    "    return 0.5*(y - X)**2  # Return Loss\n",
    "\n",
    "\n",
    "def backprop_rec(i, X, y):\n",
    "    global a, z, w, b, n_weights, n_bias\n",
    "\n",
    "    # Base Case\n",
    "    if i+1 > len(w): return (X - y).reshape(1,-1).T\n",
    "    \n",
    "    g = backprop_rec(i+1, X, y) * relu(z[i], True)  # Get Next Layer Derivative\n",
    "    \n",
    "    # Derivative with respect to weight [1xn]  \n",
    "    if i-1 < 0: w_der = y.reshape(1,-1).T  # Input Matrix\n",
    "    else: w_der = a[i-1].reshape(1,-1).T  # Previous Layer Activation\n",
    "\n",
    "    # Save change in weights\n",
    "    n_weights.append(w[i] - learning_rate * (w_der @ g))\n",
    "    n_bias.append(b[i] - learning_rate * g)\n",
    "    \n",
    "    return g @ w[i].T \n",
    "\n",
    "def train_rec(epochs, size=100, threshold=0.0001):\n",
    "    l = []\n",
    "    for i in range(epochs):\n",
    "        sum_loss = 0.0\n",
    "        for x in range(size):\n",
    "            y1 = randint(0,3)\n",
    "            \n",
    "            sum_loss += backprop_entry(forward_prop(np.array(X[y1])), np.array(y[y1]))[0]\n",
    "        l.append(sum_loss/size)\n",
    "        print(f'[Epoch {i}] Loss: {l[-1]}')\n",
    "        if l[-1] < threshold or l[-1] != l[-1]: break\n",
    "    return np.array(l).flatten()\n",
    "\n",
    "# Reinitialize Weights & Bias\n",
    "initialize(-1, 1, do_print=False)\n",
    "\n",
    "loss_over_time = train_rec(200,1000)\n",
    "\n",
    "\n",
    "print(forward_prop(np.array([[1,0], \n",
    "                             [0,1], \n",
    "                             [1,1], \n",
    "                             [0,0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "name": "Minimal Backprop.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
