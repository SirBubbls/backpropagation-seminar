{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "\n",
    "![test](https://www.i2tutorials.com/wp-content/uploads/2019/09/Deep-learning-25-i2tutorials.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def relu(z, deriv=False):\n",
    "    activations = []\n",
    "    shape = z.shape\n",
    "    z = z.flatten()\n",
    "    if deriv:\n",
    "        for i in range(len(z)):\n",
    "            if z[i] >= 0:\n",
    "                activations.append(1)\n",
    "            else:\n",
    "                activations.append(-0.2)\n",
    "        return np.array(activations).reshape(shape)\n",
    "    for i in range(len(z)):\n",
    "        if z[i] > 0:\n",
    "            activations.append(z[i])\n",
    "        else:\n",
    "            activations.append(-0.2 * z[i])\n",
    "    return np.array(activations).reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight & Bias Initialization\n",
    "\n",
    "Bias Values ($b$) are initialized with $0$.  \n",
    "Weight Values ($w$) are initialized with random values between $-2$ and $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "Weights:\n",
      " [[ 0.03027841  0.10858745 -1.85944285]\n",
      " [ 0.4870347  -0.28455876  0.83378872]]\n",
      "Bias: \n",
      "[0. 0. 0.]\n",
      "\n",
      "Layer 1:\n",
      "Weights:\n",
      " [[-1.05117768  0.59218873 -1.30918413]\n",
      " [ 1.01323524 -0.05778658 -1.15917059]\n",
      " [ 0.08245821 -1.0977488  -1.00886037]]\n",
      "Bias: \n",
      "[0. 0. 0.]\n",
      "\n",
      "Layer 2:\n",
      "Weights:\n",
      " [[ 0.38640244  0.49794018]\n",
      " [ 1.13288453  1.18692924]\n",
      " [-1.09148398  2.46201963]]\n",
      "Bias: \n",
      "[0. 0.]\n",
      "\n",
      "Layer 3:\n",
      "Weights:\n",
      " [[2.16197737]\n",
      " [0.07538075]]\n",
      "Bias: \n",
      "[0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def initialize():\n",
    "    global w, b\n",
    "    w = [\n",
    "        np.array(np.random.randn(6)).reshape(2,3),\n",
    "        np.array(np.random.randn(9)).reshape(3,3),\n",
    "        np.array(np.random.randn(6)).reshape(3,2),\n",
    "        np.array(np.random.randn(2)).reshape(2,1)\n",
    "    ]\n",
    "    b = [\n",
    "        np.array(np.zeros(3)),\n",
    "        np.array(np.zeros(3)),\n",
    "        np.array(np.zeros(2)),\n",
    "        np.array(np.zeros(1))\n",
    "    ]\n",
    "\n",
    "w, b = [], []\n",
    "\n",
    "initialize()\n",
    "\n",
    "for i in range(len(b)): print(f'Layer {i}:\\nWeights:\\n {w[i]}\\nBias: \\n{b[i]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "$a$ holds each layers activation vector.  \n",
    "$z$ holds each layers pre nonlinearity vector.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "For each layer $L$, starting with $L_0$ we multiply the $h$ vector with the weight matrix $w$.\n",
    "\n",
    "$$\n",
    "w = \\left[ \\begin{array}{rrr}\n",
    "1.3 & 0.2 \\\\                                              \n",
    "0.1 & 1.4 \\\\\n",
    "1.2 & 0 \\\\\n",
    "\\end{array}\\right] \n",
    "h = \\left( \\begin{array}{rrr}\n",
    "1.3 \\\\                                              \n",
    "0.1 \\\\\n",
    "1.2 \\\\\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04201663])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, z = [], []\n",
    "\n",
    "initialize()\n",
    "\n",
    "def forward_prop(X):\n",
    "    h = X\n",
    "    global a, z\n",
    "    a,z  = [], []\n",
    "    for i in range(len(w)):\n",
    "        h = h @ w[i] # weigt * input\n",
    "        h = h + b[i] # bias add\n",
    "        z.append(h)\n",
    "        h = relu(h) # Activation Function\n",
    "        a.append(h)\n",
    "    return h\n",
    "\n",
    "forward_prop(np.array([0,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Prop\n",
    "\n",
    "for each layer\n",
    "\n",
    "$g = loss'(X,y)$\n",
    "\n",
    "## Step 1 ($a$ to $z$)\n",
    "\n",
    "$g = relu'(z)$\n",
    "\n",
    "\n",
    "\n",
    "## Step 2 ($z$ to $W$)\n",
    "\n",
    "$g = relu'(z) * a_{L-1}$\n",
    "\n",
    "# Dimensions\n",
    "\n",
    "$g = [1\\times2]$\n",
    "\n",
    "## Step 1 Activation Function Derriv\n",
    "\n",
    "$g = [1\\times2]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.50936386]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.12972577])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.2\n",
    "\n",
    "initialize()\n",
    "\n",
    "def back_prop(X, y, print_loss=False):\n",
    "    loss = 0\n",
    "    global a, z, w, b\n",
    "    \n",
    "    g = (X - y).reshape(1,-1).T\n",
    "    loss = 0.5*(y - X)**2\n",
    "    \n",
    "    if print_loss:\n",
    "        print(\"Loss: \", (y - X)**2)\n",
    "    \n",
    "    n_weights, n_bias = [], []\n",
    "    \n",
    "    for x in range(len(w)):\n",
    "        i = len(b) - 1 - x\n",
    "\n",
    "        # Activation Function Derrivative [1xn]\n",
    "        g = g * relu(z[i], True)  # Activation Function Derriv\n",
    "        \n",
    "        # Derivative with respect to weight [1xn]  \n",
    "        if i-1 < 0: w_der = y.reshape(1,-1).T\n",
    "        else: w_der = a[i-1].reshape(1,-1).T  # Previous Layer Activation\n",
    "        \n",
    "        \n",
    "#         print(w_der.shape, g.shape)\n",
    "#         print((w_der @ g).shape)\n",
    "        \n",
    "        # Change in Weights\n",
    "        new_weights = w[i] - learning_rate * (w_der @ g)\n",
    "        n_weights.append(new_weights)\n",
    "        \n",
    "        new_bias = b[i] - learning_rate * g\n",
    "        n_bias.append(new_bias)\n",
    "        \n",
    "        g = g @ w[i].T \n",
    "    \n",
    "    n_weights = list(reversed(n_weights))\n",
    "    n_bias = list(reversed(n_bias))\n",
    "    w = n_weights\n",
    "    b = n_bias\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "\n",
    "X = forward_prop(np.array([0,1]))\n",
    "print(X)\n",
    "back_prop(X, np.array([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [0.0421284]\n",
      "Loss:  [0.01402559]\n",
      "Loss:  [0.01092947]\n",
      "Loss:  [0.00788807]\n",
      "Loss:  [0.00434759]\n",
      "Loss:  [0.00404377]\n",
      "Loss:  [0.00295679]\n",
      "Loss:  [0.00194217]\n",
      "Loss:  [0.00129463]\n",
      "Loss:  [0.00123925]\n",
      "Loss:  [0.00078409]\n",
      "Loss:  [0.00030952]\n",
      "Loss:  [0.00025254]\n",
      "Loss:  [0.0001971]\n",
      "Loss:  [0.00011784]\n",
      "Loss:  [0.00010268]\n",
      "Loss:  [5.01842126e-05]\n",
      "Loss:  [1.40237842e-05]\n",
      "Loss:  [1.33957498e-05]\n",
      "Loss:  [1.87633662e-05]\n",
      "[[0.49987784]]\n",
      "[[0.89843738]]\n",
      "[[0.99728911]]\n",
      "[[1.99931765]]\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "def train(epochs, size=100):\n",
    "    for i in range(epochs):\n",
    "        tmp = 0.0\n",
    "        for x in range(size):\n",
    "            y1 = randint(0,1)\n",
    "            y2 = randint(0,1)\n",
    "            tmp += back_prop(forward_prop(np.array([y1,y2])), np.array([y1+y2]))[0]\n",
    "        print(\"Loss: \", tmp/size)\n",
    "\n",
    "initialize()\n",
    "train(20, 100)\n",
    "print(forward_prop(np.array([0.2,0.3])))\n",
    "print(forward_prop(np.array([0.7,0.2])))\n",
    "print(forward_prop(np.array([1,0])))\n",
    "print(forward_prop(np.array([1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [0.11587483]\n",
      "Loss:  [0.0589459]\n",
      "Loss:  [0.05006279]\n",
      "Loss:  [0.05050994]\n",
      "Loss:  [0.06573703]\n",
      "Loss:  [0.04487583]\n",
      "Loss:  [0.05829067]\n",
      "Loss:  [0.14054139]\n",
      "Loss:  [0.12994083]\n",
      "Loss:  [0.10930429]\n",
      "Loss:  [0.12711978]\n",
      "Loss:  [0.10554828]\n",
      "Loss:  [0.10320527]\n",
      "Loss:  [0.06369206]\n",
      "Loss:  [0.05432755]\n",
      "Loss:  [0.05866766]\n",
      "Loss:  [0.04947216]\n",
      "Loss:  [0.04313845]\n",
      "Loss:  [0.05004426]\n",
      "Loss:  [0.03349359]\n",
      "[[0.68057903]]\n",
      "[[1.3514689]]\n",
      "[[0.2796302]]\n",
      "[[0.37055563]]\n",
      "[array([[-1.05005165, -1.25493323, -2.17923395],\n",
      "       [-1.89874884, -0.29496835, -0.53262792]]), array([[ 0.59964667, -1.32177852, -0.368207  ],\n",
      "       [-1.02396719,  0.81586807, -0.68092178],\n",
      "       [-1.22883445, -1.07504155, -3.85775008]]), array([[-0.8679731 ,  0.33597774],\n",
      "       [ 1.63709093,  0.08803059],\n",
      "       [ 0.94935286, -3.12693656]]), array([[ 1.01462831],\n",
      "       [-3.42823431]])]\n",
      "[array([[-0.9538185 ,  0.24041208,  0.57860038]]), array([[ 0.28061212, -0.11642197, -0.08085342]]), array([[-0.19291753,  1.89210103]]), array([[-0.65441656]])]\n"
     ]
    }
   ],
   "source": [
    "X = [\n",
    "    [0,1],\n",
    "    [0,0],\n",
    "    [1,0],\n",
    "    [1,1]\n",
    "]\n",
    "\n",
    "y = [1,0,1,0]\n",
    "\n",
    "initialize()\n",
    "\n",
    "def train(epochs, size=100):\n",
    "    for i in range(epochs):\n",
    "        tmp = 0.0\n",
    "        for x in range(size):\n",
    "            y1 = randint(0,3)\n",
    "            tmp += back_prop(forward_prop(np.array(X[y1])), np.array(y[y1]))[0]\n",
    "        print(\"Loss: \", tmp/size)\n",
    "      \n",
    "\n",
    "train(20, 100)\n",
    "\n",
    "print(forward_prop(np.array([1,0])))\n",
    "print(forward_prop(np.array([0,1])))\n",
    "print(forward_prop(np.array([0,0])))\n",
    "print(forward_prop(np.array([1,1])))\n",
    "\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w': [array([[-1.05005165, -1.25493323, -2.17923395],\n",
      "       [-1.89874884, -0.29496835, -0.53262792]]), array([[ 0.59964667, -1.32177852, -0.368207  ],\n",
      "       [-1.02396719,  0.81586807, -0.68092178],\n",
      "       [-1.22883445, -1.07504155, -3.85775008]]), array([[-0.8679731 ,  0.33597774],\n",
      "       [ 1.63709093,  0.08803059],\n",
      "       [ 0.94935286, -3.12693656]]), array([[ 1.01462831],\n",
      "       [-3.42823431]])], 'b': [array([[-0.9538185 ,  0.24041208,  0.57860038]]), array([[ 0.28061212, -0.11642197, -0.08085342]]), array([[-0.19291753,  1.89210103]]), array([[-0.65441656]])]}\n"
     ]
    }
   ],
   "source": [
    "working_w_b = {\n",
    "    \"w\": w,\n",
    "    \"b\": b\n",
    "}\n",
    "\n",
    "print(working_w_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.68057903]]\n",
      "[[1.3514689]]\n",
      "[[0.2796302]]\n",
      "[[0.37055563]]\n",
      "[array([[-1.05005165, -1.25493323, -2.17923395],\n",
      "       [-1.89874884, -0.29496835, -0.53262792]]), array([[ 0.59964667, -1.32177852, -0.368207  ],\n",
      "       [-1.02396719,  0.81586807, -0.68092178],\n",
      "       [-1.22883445, -1.07504155, -3.85775008]]), array([[-0.8679731 ,  0.33597774],\n",
      "       [ 1.63709093,  0.08803059],\n",
      "       [ 0.94935286, -3.12693656]]), array([[ 1.01462831],\n",
      "       [-3.42823431]])]\n",
      "[array([[-0.9538185 ,  0.24041208,  0.57860038]]), array([[ 0.28061212, -0.11642197, -0.08085342]]), array([[-0.19291753,  1.89210103]]), array([[-0.65441656]])]\n"
     ]
    }
   ],
   "source": [
    "w = working_w_b[\"w\"]\n",
    "b = working_w_b[\"b\"]\n",
    "\n",
    "print(forward_prop(np.array([1,0])))\n",
    "print(forward_prop(np.array([0,1])))\n",
    "print(forward_prop(np.array([0,0])))\n",
    "print(forward_prop(np.array([1,1])))\n",
    "\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "name": "Minimal Backprop.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
