{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "\n",
    "![test](https://www.i2tutorials.com/wp-content/uploads/2019/09/Deep-learning-25-i2tutorials.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def relu(z, deriv=False):\n",
    "    activations = []\n",
    "    shape = z.shape\n",
    "    z = z.flatten()\n",
    "    if deriv:\n",
    "        for i in range(len(z)):\n",
    "            if z[i] >= 0:\n",
    "                activations.append(1)\n",
    "            else:\n",
    "                activations.append(-0.2)\n",
    "        return np.array(activations).reshape(shape)\n",
    "    for i in range(len(z)):\n",
    "        if z[i] > 0:\n",
    "            activations.append(z[i])\n",
    "        else:\n",
    "            activations.append(-0.2 * z[i])\n",
    "    return np.array(activations).reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight & Bias Initialization\n",
    "\n",
    "Bias Values ($b$) are initialized with $0$.  \n",
    "Weight Values ($w$) are initialized with random values between $-2$ and $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "Weights:\n",
      " [[0.99518784 0.06625664 0.9412545 ]\n",
      " [0.60178277 0.41101902 0.46634165]]\n",
      "Bias: \n",
      "[0. 0. 0.]\n",
      "\n",
      "Layer 1:\n",
      "Weights:\n",
      " [[0.36098652 0.5992778  0.61344756]\n",
      " [0.36672786 0.16745812 0.93395789]\n",
      " [0.23537374 0.60426969 0.28901035]]\n",
      "Bias: \n",
      "[0. 0. 0.]\n",
      "\n",
      "Layer 2:\n",
      "Weights:\n",
      " [[0.36353909 0.27046865]\n",
      " [0.10499192 0.92876376]\n",
      " [0.42341423 0.54617965]]\n",
      "Bias: \n",
      "[0. 0.]\n",
      "\n",
      "Layer 3:\n",
      "Weights:\n",
      " [[0.32089119]\n",
      " [0.55326808]]\n",
      "Bias: \n",
      "[0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def initialize(min=0, max=1, do_print=True):\n",
    "    global w, b\n",
    "    w = [\n",
    "            max * np.random.uniform(min, max, (2, 3)),\n",
    "            max * np.random.uniform(min, max, (3, 3)),\n",
    "            max * np.random.uniform(min, max, (3, 2)),\n",
    "            max * np.random.uniform(min, max, (2, 1))\n",
    "        ]\n",
    "    b = [\n",
    "        np.array(np.zeros(3)),\n",
    "        np.array(np.zeros(3)),\n",
    "        np.array(np.zeros(2)),\n",
    "        np.array(np.zeros(1))\n",
    "    ]\n",
    "    if do_print:\n",
    "        for i in range(len(b)): print(f'Layer {i}:\\nWeights:\\n {w[i]}\\nBias: \\n{b[i]}\\n')    \n",
    "\n",
    "w, b = [], []\n",
    "\n",
    "initialize()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "$a$ holds each layers activation vector.  \n",
    "$z$ holds each layers pre nonlinearity vector.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "For each layer $L$, starting with $L_0$ we multiply the $h$ vector with the weight matrix $w$.\n",
    "\n",
    "$$\n",
    "w = \\left[ \\begin{array}{rrr}\n",
    "1.3 & 0.2 \\\\                                              \n",
    "0.1 & 1.4 \\\\\n",
    "1.2 & 0 \\\\\n",
    "\\end{array}\\right] \\ \\ \\ \\ \\ \\ \\ \n",
    "h = \\left( \\begin{array}{rrr}\n",
    "1.3 \\\\                                              \n",
    "0.1 \\\\\n",
    "1.2 \\\\\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "Weights:\n",
      " [[0.32536823 0.04899932 0.14864151]\n",
      " [0.94075787 0.7074265  0.56261487]]\n",
      "Bias: \n",
      "[0. 0. 0.]\n",
      "\n",
      "Layer 1:\n",
      "Weights:\n",
      " [[0.67694373 0.74869839 0.70778102]\n",
      " [0.85755744 0.17796351 0.2490161 ]\n",
      " [0.26323755 0.52519175 0.26837595]]\n",
      "Bias: \n",
      "[0. 0. 0.]\n",
      "\n",
      "Layer 2:\n",
      "Weights:\n",
      " [[0.96743212 0.05189514]\n",
      " [0.03399553 0.46182972]\n",
      " [0.44693799 0.64360076]]\n",
      "Bias: \n",
      "[0. 0.]\n",
      "\n",
      "Layer 3:\n",
      "Weights:\n",
      " [[0.9138108 ]\n",
      " [0.17136098]]\n",
      "Bias: \n",
      "[0.]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.88175523])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, z = [], []\n",
    "\n",
    "initialize()\n",
    "\n",
    "def forward_prop(X):\n",
    "    h = X\n",
    "    global a, z\n",
    "    a,z  = [], []\n",
    "    for i in range(len(w)):\n",
    "        h = h @ w[i] # weigt * input\n",
    "        h = h + b[i] # bias add\n",
    "        z.append(h)\n",
    "        h = relu(h) # Activation Function\n",
    "        a.append(h)\n",
    "    return h\n",
    "\n",
    "forward_prop(np.array([0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Prop\n",
    "\n",
    "for each layer\n",
    "\n",
    "$g = loss'(X,y)$\n",
    "\n",
    "## Step 1 ($a$ to $z$)\n",
    "\n",
    "$g = relu'(z)$\n",
    "\n",
    "\n",
    "\n",
    "## Step 2 ($z$ to $W$)\n",
    "\n",
    "$g = relu'(z) * a_{L-1}$\n",
    "\n",
    "# Dimensions\n",
    "\n",
    "$g = [1\\times2]$\n",
    "\n",
    "## Step 1 Activation Function Derriv\n",
    "\n",
    "$g = [1\\times2]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.521]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.115])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "\n",
    "initialize(do_print=False)\n",
    "\n",
    "def back_prop(X, y, print_loss=False):\n",
    "    global a, z, w, b\n",
    "\n",
    "    g = (X - y).reshape(1,-1).T\n",
    "    loss = 0.5*(y - X)**2\n",
    "    \n",
    "    if print_loss:\n",
    "        print(\"Loss: \", (y - X)**2)\n",
    "    \n",
    "    n_weights, n_bias = [], []\n",
    "    \n",
    "    for x in range(len(w)):\n",
    "        i = len(b) - 1 - x  # Reverse Index\n",
    "\n",
    "        # Activation Function Derrivative [1xn]\n",
    "        g = g * relu(z[i], True)  # Activation Function Derriv\n",
    "        \n",
    "        # Derivative with respect to weight [1xn]  \n",
    "        if i-1 < 0: w_der = y.reshape(1,-1).T\n",
    "        else: w_der = a[i-1].reshape(1,-1).T  # Previous Layer Activation\n",
    "        \n",
    "        \n",
    "#         print(w_der.shape, g.shape)\n",
    "#         print((w_der @ g).shape)\n",
    "        \n",
    "        # Change in Weights\n",
    "        new_weights = w[i] - learning_rate * (w_der @ g)\n",
    "        n_weights.append(new_weights)\n",
    "        \n",
    "        new_bias = b[i] - learning_rate * g\n",
    "        n_bias.append(new_bias)\n",
    "        \n",
    "        g = g @ w[i].T \n",
    "    \n",
    "    n_weights = list(reversed(n_weights))\n",
    "    n_bias = list(reversed(n_bias))\n",
    "    w = n_weights\n",
    "    b = n_bias\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "\n",
    "X = forward_prop(np.array([0,1]))\n",
    "print(X)\n",
    "back_prop(X, np.array([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [0.027]\n",
      "Loss:  [0.013]\n",
      "Loss:  [0.008]\n",
      "Loss:  [0.005]\n",
      "Loss:  [0.003]\n",
      "Loss:  [0.002]\n",
      "Loss:  [0.001]\n",
      "Loss:  [0.001]\n",
      "Loss:  [0.]\n",
      "Loss:  [0.]\n",
      "Loss:  [0.]\n",
      "Loss:  [7.71e-05]\n",
      "Loss:  [4.067e-05]\n",
      "Loss:  [2.535e-05]\n",
      "Loss:  [1.439e-05]\n",
      "Loss:  [1.011e-05]\n",
      "Loss:  [4.304e-06]\n",
      "Loss:  [2.405e-06]\n",
      "Loss:  [1.183e-06]\n",
      "Loss:  [6.362e-07]\n",
      "[[0.5]]\n",
      "[[0.899]]\n",
      "[[0.998]]\n",
      "[[1.999]]\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "def train(epochs, size=100):\n",
    "    for i in range(epochs):\n",
    "        tmp = 0.0\n",
    "        for x in range(size):\n",
    "            y1 = randint(0,1)\n",
    "            y2 = randint(0,1)\n",
    "            tmp += back_prop(forward_prop(np.array([y1,y2])), np.array([y1+y2]))[0]\n",
    "        print(\"Loss: \", tmp/size)\n",
    "\n",
    "initialize(do_print=False)\n",
    "train(20, 100)\n",
    "print(forward_prop(np.array([0.2,0.3])))\n",
    "print(forward_prop(np.array([0.7,0.2])))\n",
    "print(forward_prop(np.array([1,0])))\n",
    "print(forward_prop(np.array([1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "Weights:\n",
      " [[ 0.45263652 -0.36990386 -0.35335585]\n",
      " [ 0.58963526 -0.56347703  0.10687987]]\n",
      "Bias: \n",
      "[0. 0. 0.]\n",
      "\n",
      "Layer 1:\n",
      "Weights:\n",
      " [[ 0.93097644  0.51729997  0.996151  ]\n",
      " [-0.44854811 -0.11189472 -0.207422  ]\n",
      " [-0.1513764  -0.59704283  0.5785767 ]]\n",
      "Bias: \n",
      "[0. 0. 0.]\n",
      "\n",
      "Layer 2:\n",
      "Weights:\n",
      " [[ 0.56381598 -0.7113131 ]\n",
      " [-0.92314748  0.70076371]\n",
      " [-0.95961147  0.47062027]]\n",
      "Bias: \n",
      "[0. 0.]\n",
      "\n",
      "Layer 3:\n",
      "Weights:\n",
      " [[-0.01605429]\n",
      " [-0.04958998]]\n",
      "Bias: \n",
      "[0.]\n",
      "\n",
      "Loss:  [0.14418422]\n",
      "Loss:  [0.05672511]\n",
      "Loss:  [0.00667237]\n",
      "Loss:  [0.0009283]\n",
      "Loss:  [0.00010258]\n",
      "Loss:  [1.73457745e-05]\n",
      "Loss:  [5.37101084e-06]\n",
      "Loss:  [2.00479364e-06]\n",
      "Loss:  [8.07286043e-07]\n",
      "Loss:  [2.7726979e-07]\n",
      "[[0.99988251]]\n",
      "[[1.00007788]]\n",
      "[[0.00108673]]\n",
      "[[7.22759909e-05]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x116982250>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [\n",
    "    [0,1],\n",
    "    [0,0],\n",
    "    [1,0],\n",
    "    [1,1]\n",
    "]\n",
    "\n",
    "y = [1,0,1,0]\n",
    "\n",
    "initialize(-1, 1)\n",
    "\n",
    "def train(epochs, size=100):\n",
    "    l = []\n",
    "    for i in range(epochs):\n",
    "        tmp = 0.0\n",
    "        for x in range(size):\n",
    "            y1 = randint(0,3)\n",
    "            tmp += back_prop(forward_prop(np.array(X[y1])), np.array(y[y1]))[0]\n",
    "        l.append(tmp/size)\n",
    "        print(\"Loss: \", tmp/size)\n",
    "    return np.array(l).flatten()\n",
    "\n",
    "loss_over_time =  train(10,1000)\n",
    "\n",
    "print(forward_prop(np.array([1,0])))\n",
    "print(forward_prop(np.array([0,1])))\n",
    "print(forward_prop(np.array([0,0])))\n",
    "print(forward_prop(np.array([1,1])))\n",
    "\n",
    "plt.plot(loss_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.986e-01],\n",
       "       [2.410e-05],\n",
       "       [1.001e+00],\n",
       "       [4.559e-06]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_prop(np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "Weights:\n",
      " [[0.771 0.729 0.373]\n",
      " [0.272 0.665 0.537]]\n",
      "Bias: \n",
      "[0. 0. 0.]\n",
      "\n",
      "Layer 1:\n",
      "Weights:\n",
      " [[0.918 0.839 0.013]\n",
      " [0.298 0.202 0.563]\n",
      " [0.63  0.771 0.68 ]]\n",
      "Bias: \n",
      "[0. 0. 0.]\n",
      "\n",
      "Layer 2:\n",
      "Weights:\n",
      " [[0.142 0.176]\n",
      " [0.327 0.907]\n",
      " [0.705 0.31 ]]\n",
      "Bias: \n",
      "[0. 0.]\n",
      "\n",
      "Layer 3:\n",
      "Weights:\n",
      " [[0.514]\n",
      " [0.613]]\n",
      "Bias: \n",
      "[0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Backpropagation Implementation\n",
    "Exmaple: Learning XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Loss: [0.04]\n",
      "[Epoch 1] Loss: [0.088]\n",
      "[Epoch 2] Loss: [0.067]\n",
      "[Epoch 3] Loss: [0.068]\n",
      "[Epoch 4] Loss: [0.064]\n",
      "[Epoch 5] Loss: [0.061]\n",
      "[Epoch 6] Loss: [0.046]\n",
      "[Epoch 7] Loss: [0.106]\n",
      "[Epoch 8] Loss: [0.109]\n",
      "[Epoch 9] Loss: [0.011]\n",
      "[Epoch 10] Loss: [0.01]\n",
      "[Epoch 11] Loss: [5.968e-13]\n",
      "[[1.000e+00]\n",
      " [1.000e+00]\n",
      " [8.100e-12]\n",
      " [5.347e-14]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "\n",
    "def backprop_entry(X, y, print_loss=False):\n",
    "    global a, z, w, b, n_weights, n_bias\n",
    "    n_weights, n_bias = [], []\n",
    "    \n",
    "    backprop_rec(0, X, y)\n",
    "    \n",
    "    # Update Weights\n",
    "    w = list(reversed(n_weights))\n",
    "    b = list(reversed(n_bias))\n",
    "    return 0.5*(y - X)**2  # Return Loss\n",
    "\n",
    "\n",
    "def backprop_rec(i, X, y):\n",
    "    global a, z, w, b, n_weights, n_bias\n",
    "\n",
    "    # Base Case\n",
    "    if i+1 > len(w): return (X - y).reshape(1,-1).T\n",
    "    \n",
    "    g = backprop_rec(i+1, X, y) * relu(z[i], True)  # Get Next Layer Derivative\n",
    "    \n",
    "    # Derivative with respect to weight [1xn]  \n",
    "    if i-1 < 0: w_der = y.reshape(1,-1).T  # Input Matrix\n",
    "    else: w_der = a[i-1].reshape(1,-1).T  # Previous Layer Activation\n",
    "\n",
    "    # Save change in weights\n",
    "    n_weights.append(w[i] - learning_rate * (w_der @ g))\n",
    "    n_bias.append(b[i] - learning_rate * g)\n",
    "    \n",
    "    return g @ w[i].T \n",
    "\n",
    "def train_rec(epochs, size=100, threshold=0.0001):\n",
    "    l = []\n",
    "    for i in range(epochs):\n",
    "        sum_loss = 0.0\n",
    "        for x in range(size):\n",
    "            y1 = randint(0,3)\n",
    "            \n",
    "            sum_loss += backprop_entry(forward_prop(np.array(X[y1])), np.array(y[y1]))[0]\n",
    "        l.append(sum_loss/size)\n",
    "        print(f'[Epoch {i}] Loss: {l[-1]}')\n",
    "        if l[-1] < threshold: break\n",
    "    return np.array(l).flatten()\n",
    "\n",
    "initialize(-1, 1, do_print=False)\n",
    "\n",
    "loss_over_time = train_rec(200,1000)\n",
    "print(forward_prop(np.array([[1,0], [1,0], [1,1], [0,0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "name": "Minimal Backprop.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
