{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST\n",
    "$h_1=0$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def relu(z, deriv=False):\n",
    "    activations = []\n",
    "    shape = z.shape\n",
    "    z = z.flatten()\n",
    "    if deriv:\n",
    "        for i in range(len(z)):\n",
    "            if z[i] >= 0:\n",
    "                activations.append(1)\n",
    "            else:\n",
    "                activations.append(0)\n",
    "        return np.array(activations).reshape(shape)\n",
    "    for i in range(len(z)):\n",
    "        if z[i] > 0:\n",
    "            activations.append(z[i])\n",
    "        else:\n",
    "            activations.append(0)\n",
    "    return np.array(activations).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.34535875 0.46916952 0.18472229]\n",
      " [0.43623386 0.69973803 0.34003479]]\n",
      "[[0.37886188 0.39378201 0.22016079]\n",
      " [0.40543993 0.53455279 0.69302586]\n",
      " [0.44018559 0.53531516 0.10661116]]\n",
      "[[0.88913152 0.49103421]\n",
      " [0.24365102 0.11397508]\n",
      " [0.35173607 0.79227154]]\n",
      "[[0.70866006]\n",
      " [0.94313261]]\n"
     ]
    }
   ],
   "source": [
    "def reset():\n",
    "    global w, b\n",
    "    w = [\n",
    "        np.array(np.random.rand(6)).reshape(2,3),\n",
    "        np.array(np.random.rand(9)).reshape(3,3),\n",
    "        np.array(np.random.rand(6)).reshape(3,2),\n",
    "        np.array(np.random.rand(2)).reshape(2,1)\n",
    "    ]\n",
    "    b = [\n",
    "        np.array([np.zeros(3)]),\n",
    "        np.array([np.zeros(3)]),\n",
    "        np.array([np.zeros(2)]),\n",
    "        np.array([np.zeros(1)])\n",
    "    ]\n",
    "\n",
    "# [previous, neuron_count]\n",
    "w = [\n",
    "    np.array(np.random.rand(6)).reshape(2,3),\n",
    "    np.array(np.random.rand(9)).reshape(3,3),\n",
    "    np.array(np.random.rand(6)).reshape(3,2),\n",
    "    np.array(np.random.rand(2)).reshape(2,1)\n",
    "]\n",
    "b = [\n",
    "    np.array([np.random.rand(3)]),\n",
    "    np.array([np.random.rand(3)]),\n",
    "    np.array([np.random.rand(2)]),\n",
    "    np.array([np.random.rand(1)])\n",
    "]\n",
    "\n",
    "for i in w: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6657651 , 0.58009577, 0.14212619]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[0,1]]) @ w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.70369917])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = [\n",
    "    \n",
    "]\n",
    "a = [\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "def forward_prop(X):\n",
    "    h = X\n",
    "    global a, z\n",
    "    a,z  = [], []\n",
    "    for i in range(len(w)):\n",
    "        h = h @ w[i] # weigt * input\n",
    "        h = h + b[i] # bias add\n",
    "        z.append(h)\n",
    "        h = relu(h) # Activation Function\n",
    "        a.append(h)\n",
    "    return h\n",
    "\n",
    "forward_prop(np.array([0,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Prop\n",
    "\n",
    "for each layer\n",
    "\n",
    "$g = loss'(X,y)$\n",
    "\n",
    "## Step 1 ($a$ to $z$)\n",
    "\n",
    "$g = relu'(z)$\n",
    "\n",
    "\n",
    "\n",
    "## Step 2 ($z$ to $W$)\n",
    "\n",
    "$g = relu'(z) * a_{L-1}$\n",
    "\n",
    "# Dimensions\n",
    "\n",
    "$g = [1\\times2]$\n",
    "\n",
    "## Step 1 Activation Function Derriv\n",
    "\n",
    "$g = [1\\times2]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69531709]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.02\n",
    "\n",
    "reset()\n",
    "\n",
    "def back_prop(X, y, print_loss=False):\n",
    "    loss = 0\n",
    "    global a, z, w, b\n",
    "    \n",
    "    g = 2*(X - y).reshape(1,-1).T\n",
    "\n",
    "    loss = (y - X)**2\n",
    "    if print_loss:\n",
    "        print(\"Loss: \", (y - X)**2)\n",
    "    \n",
    "    n_weights, n_bias = [], []\n",
    "    for x in range(len(w)):\n",
    "        i = len(b) - 1 - x\n",
    "\n",
    "        # Activation Function Derrivative [1xn]\n",
    "        af_der = relu(z[i], True).reshape(1,-1).T  # Activation Function Derriv\n",
    "        g = g * af_der\n",
    "        \n",
    "        # Derivative with respect to weight [1xn]  \n",
    "        if i-1 < 0: w_der = y.reshape(1,-1).T\n",
    "        else: w_der = a[i-1].reshape(1,-1).T  # Previous Layer Activation\n",
    "        \n",
    "        # Saving New Weights\n",
    "        new_weights = w[i] - learning_rate * (w_der @ g.T)\n",
    "        n_weights.append(new_weights)\n",
    "        n_bias.append(b[i] - learning_rate * g.reshape(1,-1))\n",
    "\n",
    "        g = w[i] @ g\n",
    "  \n",
    "    n_bias = list(reversed(n_bias))\n",
    "    n_weights = list(reversed(n_weights))\n",
    "#     for x in b: print(x.shape)\n",
    "#     for x in n_bias: print(x.shape)\n",
    "    w = n_weights\n",
    "    b = n_bias\n",
    "    return loss\n",
    "    \n",
    "X = forward_prop(np.array([0,1]))\n",
    "back_prop(X, np.array([1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [[0.01012882]]\n",
      "Loss:  [[0.0030969]]\n",
      "Loss:  [[0.00153564]]\n",
      "Loss:  [[0.0010704]]\n",
      "Loss:  [[0.00059733]]\n",
      "Loss:  [[0.00031151]]\n",
      "Loss:  [[0.00016389]]\n",
      "Loss:  [[9.43639402e-05]]\n",
      "Loss:  [[5.51350155e-05]]\n",
      "Loss:  [[2.90632903e-05]]\n",
      "Loss:  [[1.86054637e-05]]\n",
      "Loss:  [[1.34233439e-05]]\n",
      "Loss:  [[6.97850596e-06]]\n",
      "Loss:  [[3.12532169e-06]]\n",
      "Loss:  [[1.56357969e-06]]\n",
      "Loss:  [[1.11545628e-06]]\n",
      "Loss:  [[6.49720387e-07]]\n",
      "Loss:  [[3.28417706e-07]]\n",
      "Loss:  [[1.74068532e-07]]\n",
      "Loss:  [[1.11550649e-07]]\n",
      "Loss:  [[6.29629962e-08]]\n",
      "Loss:  [[2.28582996e-08]]\n",
      "Loss:  [[1.45535779e-08]]\n",
      "Loss:  [[8.99545748e-09]]\n",
      "Loss:  [[4.46973198e-09]]\n",
      "Loss:  [[2.09412185e-09]]\n",
      "Loss:  [[1.12883647e-09]]\n",
      "Loss:  [[5.55715176e-10]]\n",
      "Loss:  [[2.59822014e-10]]\n",
      "Loss:  [[1.8439068e-10]]\n",
      "Loss:  [[1.02547773e-10]]\n",
      "Loss:  [[5.24751536e-11]]\n",
      "Loss:  [[3.27699111e-11]]\n",
      "Loss:  [[1.93033879e-11]]\n",
      "Loss:  [[1.38349843e-11]]\n",
      "Loss:  [[5.59829559e-12]]\n",
      "Loss:  [[3.09528791e-12]]\n",
      "Loss:  [[1.75731129e-12]]\n",
      "Loss:  [[8.52069558e-13]]\n",
      "Loss:  [[5.61036056e-13]]\n",
      "[[0.50000039]]\n",
      "[[0.90000019]]\n",
      "[[0.99999988]]\n",
      "[[2.00000124]]\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "reset()\n",
    "\n",
    "def train(epochs, size=100):\n",
    "    for i in range(epochs):\n",
    "        tmp = 0.0\n",
    "        for x in range(size):\n",
    "            y1 = randint(0,1)\n",
    "            y2 = randint(0,1)\n",
    "            tmp += back_prop(forward_prop(np.array([y1,y2])), np.array([y1+y2])) \n",
    "        print(\"Loss: \", tmp/size)\n",
    "\n",
    "train(40, 100)\n",
    "print(forward_prop(np.array([0.2,0.3])))\n",
    "print(forward_prop(np.array([0.7,0.2])))\n",
    "print(forward_prop(np.array([1,0])))\n",
    "print(forward_prop(np.array([1,1])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.44671871e-04, 9.67246506e-05, 1.85768508e-04]]), array([[ 5.30291522e-05, -1.06914399e-04,  5.60509843e-05]]), array([[-0.00031825, -0.00087005]]), array([[-0.00012256]])]\n"
     ]
    }
   ],
   "source": [
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [[0.06709958]]\n",
      "Loss:  [[1.98819205e-09]]\n",
      "Loss:  [[3.42828565e-19]]\n",
      "Loss:  [[3.97647853e-29]]\n",
      "Loss:  [[2.93843467e-31]]\n",
      "Loss:  [[1.91873153e-31]]\n",
      "Loss:  [[1.58143843e-31]]\n",
      "Loss:  [[1.66801606e-31]]\n",
      "Loss:  [[1.71022915e-31]]\n",
      "Loss:  [[1.94534691e-31]]\n",
      "Loss:  [[2.39242653e-31]]\n",
      "Loss:  [[1.97108019e-31]]\n",
      "Loss:  [[2.14980413e-31]]\n",
      "Loss:  [[2.00562581e-31]]\n",
      "Loss:  [[1.82810845e-31]]\n",
      "Loss:  [[2.92957116e-31]]\n",
      "Loss:  [[2.79730578e-31]]\n",
      "Loss:  [[1.7166716e-31]]\n",
      "Loss:  [[1.6312734e-31]]\n",
      "Loss:  [[1.56257413e-31]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[6.00954158e-16]]\n",
      "[[0]]\n"
     ]
    }
   ],
   "source": [
    "X = [\n",
    "    [0,1],\n",
    "    [0,0],\n",
    "    [1,0],\n",
    "    [1,1]\n",
    "]\n",
    "\n",
    "y = [1,0,1,0]\n",
    "\n",
    "reset()\n",
    "\n",
    "def train(epochs, size=100):\n",
    "    for i in range(epochs):\n",
    "        tmp = 0.0\n",
    "        for x in range(size):\n",
    "            y1 = randint(0,3)\n",
    "            tmp += back_prop(forward_prop(np.array(X[y1])), np.array(y[y1])) \n",
    "        print(\"Loss: \", tmp/size)\n",
    "      \n",
    "train(20, 2000)\n",
    "\n",
    "print(forward_prop(np.array([1,0])))\n",
    "print(forward_prop(np.array([0,1])))\n",
    "print(forward_prop(np.array([0,0])))\n",
    "print(forward_prop(np.array([1,1])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "name": "Minimal Backprop.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
