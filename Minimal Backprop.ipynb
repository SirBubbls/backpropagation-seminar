{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST\n",
    "$h_1=0$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def relu(z, deriv=False):\n",
    "    activations = []\n",
    "    shape = z.shape\n",
    "    z = z.flatten()\n",
    "    if deriv:\n",
    "        for i in range(len(z)):\n",
    "            if z[i] >= 0:\n",
    "                activations.append(1)\n",
    "            else:\n",
    "                activations.append(0)\n",
    "        return np.array(activations).reshape(shape)\n",
    "    for i in range(len(z)):\n",
    "        if z[i] > 0:\n",
    "            activations.append(z[i])\n",
    "        else:\n",
    "            activations.append(0)\n",
    "    return np.array(activations).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26754037 0.4141417  0.63573997]\n",
      " [0.22918934 0.94005709 0.04360092]]\n",
      "[[0.93913987 0.90593735 0.62291966]\n",
      " [0.11355432 0.21802018 0.12834518]\n",
      " [0.1881299  0.48095143 0.54786501]]\n",
      "[[0.51232152 0.38836737]\n",
      " [0.96255943 0.02365192]\n",
      " [0.32203345 0.76508755]]\n",
      "[[0.96871103]\n",
      " [0.09930695]]\n"
     ]
    }
   ],
   "source": [
    "def reset():\n",
    "    global w, b\n",
    "    w = [\n",
    "        np.array(np.random.rand(6)).reshape(2,3),\n",
    "        np.array(np.random.rand(9)).reshape(3,3),\n",
    "        np.array(np.random.rand(6)).reshape(3,2),\n",
    "        np.array(np.random.rand(2)).reshape(2,1)\n",
    "    ]\n",
    "    b = [\n",
    "        np.array(np.random.rand(3)),\n",
    "        np.array(np.random.rand(3)),\n",
    "        np.array(np.random.rand(2)),\n",
    "        np.array(np.random.rand(1))\n",
    "    ]\n",
    "\n",
    "# [previous, neuron_count]\n",
    "w = [\n",
    "    np.array(np.random.rand(6)).reshape(2,3),\n",
    "    np.array(np.random.rand(9)).reshape(3,3),\n",
    "    np.array(np.random.rand(6)).reshape(3,2),\n",
    "    np.array(np.random.rand(2)).reshape(2,1)\n",
    "]\n",
    "b = [\n",
    "    np.array(np.random.rand(3)),\n",
    "    np.array(np.random.rand(3)),\n",
    "    np.array(np.random.rand(2)),\n",
    "    np.array(np.random.rand(1))\n",
    "]\n",
    "\n",
    "for i in w: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21441747, 0.58068606, 0.85106577]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[0,1]]) @ w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31.33006967])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = [\n",
    "    \n",
    "]\n",
    "a = [\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "def forward_prop(X):\n",
    "    h = X\n",
    "    global a, z\n",
    "    a,z  = [], []\n",
    "    for i in range(len(w)):\n",
    "        h = h @ w[i] # weigt * input\n",
    "        h = h + b[i] # bias add\n",
    "        z.append(h)\n",
    "        h = relu(h) # Activation Function\n",
    "        a.append(h)\n",
    "    return h\n",
    "\n",
    "forward_prop(np.array([0,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Prop\n",
    "\n",
    "for each layer\n",
    "\n",
    "$g = loss'(X,y)$\n",
    "\n",
    "## Step 1 ($a$ to $z$)\n",
    "\n",
    "$g = relu'(z)$\n",
    "\n",
    "\n",
    "\n",
    "## Step 2 ($z$ to $W$)\n",
    "\n",
    "$g = relu'(z) * a_{L-1}$\n",
    "\n",
    "# Dimensions\n",
    "\n",
    "$g = [1\\times2]$\n",
    "\n",
    "## Step 1 Activation Function Derriv\n",
    "\n",
    "$g = [1\\times2]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.8424545]\n",
      "[2.71496222]\n",
      "[2.60433369]\n",
      "[2.50723813]\n",
      "[2.42119503]\n",
      "[2.34431403]\n",
      "[2.27512598]\n",
      "[2.21246965]\n",
      "[2.15541355]\n",
      "[2.10320076]\n",
      "[2.0552091]\n",
      "[2.01092185]\n",
      "[1.96990593]\n",
      "[1.93179531]\n",
      "[1.89627827]\n",
      "[1.86308753]\n",
      "[1.83199241]\n",
      "[1.80279272]\n",
      "[1.77531373]\n",
      "[1.74940223]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.002\n",
    "\n",
    "reset()\n",
    "\n",
    "def back_prop(X, y, print_loss=False):\n",
    "    loss = 0\n",
    "    global a, z, w\n",
    "    \n",
    "    g = 2*(X - y).reshape(1,-1).T\n",
    "\n",
    "    loss = (y - X)**2\n",
    "    if print_loss:\n",
    "        print(\"Loss: \", (y - X)**2)\n",
    "    \n",
    "    n_weights = []\n",
    "    for x in range(len(w)):\n",
    "        i = len(b) - 1 - x\n",
    "\n",
    "        # Activation Function Derrivative [1xn]\n",
    "        af_der = relu(z[i], True).reshape(1,-1).T  # Activation Function Derriv\n",
    "        g = g * af_der\n",
    "        \n",
    "        \n",
    "        # Derivative with respect to weight [1xn]  \n",
    "        if i-1 < 0: w_der = y.reshape(1,-1).T\n",
    "        else: w_der = a[i-1].reshape(1,-1).T  # Previous Layer Activation\n",
    "        \n",
    "        # Change in Weights\n",
    "        scalar = g\n",
    "        # Saving New Weights\n",
    "        new_weights = w[i] - learning_rate * (w_der @ g.T)\n",
    "        n_weights.append(new_weights)\n",
    "        g = w[i] @ g\n",
    "    \n",
    "    n_weights = list(reversed(n_weights))\n",
    "    w = n_weights\n",
    "    return loss\n",
    "    \n",
    "\n",
    "#         print(\"\\nOld Weights: \", w[i-1], w[i-1].shape)\n",
    "#         print(\"\\nNew Weights: \", new_weights, new_weights.shape)\n",
    "#         print(\"############################################################\")\n",
    "\n",
    "for i in range(20):\n",
    "    X = forward_prop(np.array([0,1]))\n",
    "    print(X)\n",
    "    back_prop(X, np.array([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [0.64574296]\n",
      "Loss:  [0.39980032]\n",
      "Loss:  [0.45976417]\n",
      "Loss:  [0.48856786]\n",
      "Loss:  [0.50825224]\n",
      "Loss:  [0.47954362]\n",
      "Loss:  [0.39296417]\n",
      "Loss:  [0.42215777]\n",
      "Loss:  [0.40160614]\n",
      "Loss:  [0.32443555]\n",
      "Loss:  [0.36735106]\n",
      "Loss:  [0.27118338]\n",
      "Loss:  [0.28982061]\n",
      "Loss:  [0.21946767]\n",
      "Loss:  [0.17738216]\n",
      "Loss:  [0.13553399]\n",
      "Loss:  [0.13036952]\n",
      "Loss:  [0.08162366]\n",
      "Loss:  [0.06296159]\n",
      "Loss:  [0.05615115]\n",
      "Loss:  [0.02666766]\n",
      "Loss:  [0.02569117]\n",
      "Loss:  [0.01619464]\n",
      "Loss:  [0.0082117]\n",
      "Loss:  [0.00719509]\n",
      "Loss:  [0.00493777]\n",
      "Loss:  [0.00356099]\n",
      "Loss:  [0.00242509]\n",
      "Loss:  [0.00274931]\n",
      "Loss:  [0.00259351]\n",
      "Loss:  [0.00203652]\n",
      "Loss:  [0.00237643]\n",
      "Loss:  [0.0021581]\n",
      "Loss:  [0.00221825]\n",
      "Loss:  [0.0018314]\n",
      "Loss:  [0.00205594]\n",
      "Loss:  [0.00256404]\n",
      "Loss:  [0.00217049]\n",
      "Loss:  [0.00205945]\n",
      "Loss:  [0.00180026]\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "reset()\n",
    "\n",
    "def train(epochs, size=100):\n",
    "    for i in range(epochs):\n",
    "        tmp = 0.0\n",
    "        for x in range(size):\n",
    "            y1 = randint(0,1)\n",
    "            y2 = randint(0,1)\n",
    "            tmp += back_prop(forward_prop(np.array([y1,y2])), np.array([y1+y2])) \n",
    "        print(\"Loss: \", tmp/size)\n",
    "      \n",
    "train(40, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51980858]\n",
      "[0.8844378]\n",
      "[0.95490285]\n",
      "[2.01427766]\n"
     ]
    }
   ],
   "source": [
    "print(forward_prop(np.array([0.2,0.3])))\n",
    "print(forward_prop(np.array([0.7,0.2])))\n",
    "print(forward_prop(np.array([1,0])))\n",
    "print(forward_prop(np.array([1,1])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [0.4879567]\n",
      "Loss:  [0.25986924]\n",
      "Loss:  [0.25376168]\n",
      "Loss:  [0.25609294]\n",
      "Loss:  [0.25265645]\n",
      "Loss:  [0.23578726]\n",
      "Loss:  [0.2267871]\n",
      "Loss:  [0.20987972]\n",
      "Loss:  [0.19538341]\n",
      "Loss:  [0.18155554]\n",
      "Loss:  [0.17575018]\n",
      "Loss:  [0.1799497]\n",
      "Loss:  [0.17179148]\n",
      "Loss:  [0.17219131]\n",
      "Loss:  [0.16790021]\n",
      "Loss:  [0.15809177]\n",
      "Loss:  [0.15632871]\n",
      "Loss:  [0.15152383]\n",
      "Loss:  [0.15199152]\n",
      "Loss:  [0.15513698]\n",
      "Loss:  [0.17584591]\n",
      "Loss:  [0.17684867]\n",
      "Loss:  [0.17620493]\n",
      "Loss:  [0.15471214]\n",
      "Loss:  [0.15265165]\n",
      "Loss:  [0.14830207]\n",
      "Loss:  [0.1525435]\n",
      "Loss:  [0.15116165]\n",
      "Loss:  [0.17963072]\n",
      "Loss:  [0.17397263]\n",
      "Loss:  [0.16591043]\n",
      "Loss:  [0.16880415]\n",
      "Loss:  [0.17180674]\n",
      "Loss:  [0.1585868]\n",
      "Loss:  [0.16664021]\n",
      "Loss:  [0.17319196]\n",
      "Loss:  [0.16521052]\n",
      "Loss:  [0.16584363]\n",
      "Loss:  [0.16095915]\n",
      "Loss:  [0.16890004]\n",
      "Loss:  [0.16273996]\n",
      "Loss:  [0.16492065]\n",
      "Loss:  [0.16227448]\n",
      "Loss:  [0.1647396]\n",
      "Loss:  [0.17453722]\n",
      "Loss:  [0.1676691]\n",
      "Loss:  [0.16467034]\n",
      "Loss:  [0.16388349]\n",
      "Loss:  [0.15894318]\n",
      "Loss:  [0.16847701]\n",
      "Loss:  [0.17016912]\n",
      "Loss:  [0.15927554]\n",
      "Loss:  [0.16891639]\n",
      "Loss:  [0.17162211]\n",
      "Loss:  [0.16380692]\n",
      "Loss:  [0.16883779]\n",
      "Loss:  [0.17145898]\n",
      "Loss:  [0.16557238]\n",
      "Loss:  [0.1624916]\n",
      "Loss:  [0.16155597]\n",
      "Loss:  [0.15722101]\n",
      "Loss:  [0.16597459]\n",
      "Loss:  [0.16773255]\n",
      "Loss:  [0.17222796]\n",
      "Loss:  [0.17163246]\n",
      "Loss:  [0.16994092]\n",
      "Loss:  [0.16582027]\n",
      "Loss:  [0.1690694]\n",
      "Loss:  [0.1661284]\n",
      "Loss:  [0.17258147]\n",
      "Loss:  [0.16164145]\n",
      "Loss:  [0.16741419]\n",
      "Loss:  [0.1678926]\n",
      "Loss:  [0.16562815]\n",
      "Loss:  [0.18109741]\n",
      "Loss:  [0.16247471]\n",
      "Loss:  [0.17097624]\n",
      "Loss:  [0.1694173]\n",
      "Loss:  [0.18024956]\n",
      "Loss:  [0.15228915]\n",
      "Loss:  [0.17284245]\n",
      "Loss:  [0.17057737]\n",
      "Loss:  [0.16587409]\n",
      "Loss:  [0.17279182]\n",
      "Loss:  [0.17589475]\n",
      "Loss:  [0.16380531]\n",
      "Loss:  [0.16002815]\n",
      "Loss:  [0.17263268]\n",
      "Loss:  [0.169875]\n",
      "Loss:  [0.1646048]\n",
      "Loss:  [0.16721741]\n",
      "Loss:  [0.1646864]\n",
      "Loss:  [0.1688465]\n",
      "Loss:  [0.17122061]\n",
      "Loss:  [0.16971434]\n",
      "Loss:  [0.15994332]\n",
      "Loss:  [0.16463699]\n",
      "Loss:  [0.16155581]\n",
      "Loss:  [0.16563771]\n",
      "Loss:  [0.16525259]\n",
      "[0.68943037]\n",
      "[0.6988223]\n",
      "[0]\n",
      "[0.693703]\n"
     ]
    }
   ],
   "source": [
    "X = [\n",
    "    [0,1],\n",
    "    [0,0],\n",
    "    [1,0],\n",
    "    [1,1]\n",
    "]\n",
    "\n",
    "y = [1,0,1,0]\n",
    "\n",
    "reset()\n",
    "\n",
    "def train(epochs, size=100):\n",
    "    for i in range(epochs):\n",
    "        tmp = 0.0\n",
    "        for x in range(size):\n",
    "            y1 = randint(0,3)\n",
    "            tmp += back_prop(forward_prop(np.array(X[y1])), np.array(y[y1])) \n",
    "        print(\"Loss: \", tmp/size)\n",
    "      \n",
    "train(100, 1000)\n",
    "\n",
    "print(forward_prop(np.array([1,0])))\n",
    "print(forward_prop(np.array([0,1])))\n",
    "print(forward_prop(np.array([0,0])))\n",
    "print(forward_prop(np.array([1,1])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "name": "Minimal Backprop.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
