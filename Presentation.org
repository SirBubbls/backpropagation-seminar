#+bind: org-export-publishing-directory "./exports"
#+TITLE: Backpropagation
#+LANGUAGE: de
#+EXPORT_FILE_NAME: docs/index.html
#+AUTHOR: Lucas Sas Brunschier
#+DATE: SS20
#+EMAIL: lucassas@live.de
#+OPTIONS: toc:nil num:nil timestamp:nil
#+REVEAL_EXTRA_CSS: style.css
#+STYLE: <link rel="stylesheet" type="text/css" href="style.css" />
#+REVEAL_ROOT: reveal
#+REVEAL_THEME: solarized

* Intro
Präsentation ist auch online [[https://sirbubbls.github.io/backpropagation-seminar][sirbubbls.github.io/backpropagation-seminar]]

Präsentation ist in Org-Mode geschrieben, also sourcen aller Grafiken und
Beispiele sind integriert.
** Zusätzliche Ressourcen
Deep Learning (Ian Goodfellow, Yoshua Bengio & Aaron Courville)
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 150
[[https://images-eu.ssl-images-amazon.com/images/I/610HnULa0dL._SY445_QL70_ML2_.jpg]]

https://www.deeplearningbook.org

[[https://www.deeplearningbook.org/contents/mlp.html][Backpropagation Kapitel]]

** Jupyter Notebook
Beispiele für alle Methoden dieser Präsentation sind in diesem [[https://github.com/SirBubbls/backpropagation-seminar/blob/master/Backpropagation.ipynb][IPython Notebook]] zu finden.
* Inhalt
1. Neuronale Netze
2. Vorwissen
3. Forward Propagation
4. Backpropagation
5. General Backpropagation
  
* Neuronale Netze
Formale Definition für ein neuronales Netz: $y=f(x; \theta)$ und $y=f^*(x)$
- $y$ ist den Wert den unser NN vorraussagen soll
- $x$ sind die Input Daten, die das NN erhält
- $\theta$ sind Parameter des neuronalen Netzes, um $f$ so nah wie möglich an
  die optimale Funktion $f^*$ anzunähern.
** Wie ist nun ein neurales Netzwerk aufgebaut?
Wir teilen das Netzwerk in Schichten (Layer) auf.

#+ATTR_HTML: :width 50% :height 50%
https://upload.wikimedia.org/wikipedia/commons/thumb/3/3d/Neural_network.svg/1200px-Neural_network.svg.png

Jeder Layer bildet eine Funktion $f^{i}$, mit $i=Layer\ Index$ ab.

*** Formell
Somit ist ein neurales Netzwerk eine Kette an Funktionen $f$.
#+begin_quote
Ein Netz mit $3$ Layern wäre somit $f^2(f^1(f^0(X)))$
mit $X=Input\ Data$
#+end_quote
*** Aufbau eines Layers
Jeder Layer enthält mindestens folgende Informationen:
- Eine Weight Matrix ($w$)
- Einen Bias Vektor ($b$)
*** Aktivierungsfunktion
Da wir bei Neural Networks oft versuchen non-lineare Zusammenhänge zu aproximieren, benötigen wir auch eine nicht-lineare Komponente in unserem NN.
*** Beliebte Aktivierungsfunktionen
- Rectified Linear Unit ($ReLU$)
- $Leaky\ ReLU$
- Sigmoid Function
*** Cost Function ($J$)
Eine Funktion um zu bestimmen wie 'nah' wir uns an unserem erwarteten Inference Wert befinden.
#+begin_quote
In dieser Präsentation benutzen wir die Euklidean-Distance $(x-y)^2$ als Cost Function.
#+end_quote
* Historisches
Erste Verwendung in Zusammenhang mit artificial neural networks im Jahre 1985 im Buch 'Learning Logic' (Parker, D.B.).

* Vorwissen
** Gradient
Der Gradient ist der Vektor aller partiellen Ableitungen einer Funktion $f$.
#+begin_quote
Notation: $\nabla_xf(x)$
#+end_quote
*** Beispiel
#+begin_quote
$f(x, y) = 2x^2 + y^3$
#+end_quote
$\rightarrow \nabla_xf(x)=\left(\begin{array}{c} f'_x \\ f'_y \end{array}\right)= \left(\begin{array}{c} 4x \\ 3y \end{array}\right)$
** Stochastic Gradient Descent
Der Gradient Descent Algorithmus wird dafür verwendet ein lokales Minimum einer Funktion zu bestimmen.
*** Beispiel
Funktion $f(x)=x_1^2-x_2^2$ ist gegeben. \\
Also: $\nabla_xf(x)=\left(\begin{array}{c} f'_{x_1} \\ f'_{x_2} \end{array}\right)= \left(\begin{array}{c} 2x_1 \\ -2x_2 \end{array}\right)$ \\
Wir starten mit einem beliebigen Punkt: z.B. $\left(\begin{array}{c} 2 \\ 1 \end{array}\right)$ und setzen ein: \\
$\left(\begin{array}{c} 2x_1 \\ -2x_2 \end{array}\right) = \left(\begin{array}{c} 2 * 2 \\ -2 * 1 \end{array}\right) = \left(\begin{array}{c} 4 \\ -2 \end{array}\right)$

$Neuer\ Punkt = \left(\begin{array}{c} 2 \\ 1 \end{array}\right)+ \lambda \left(\begin{array}{c} 4 \\ -2 \end{array}\right)$ mit $\lambda: learning\ rate$

** Computational Graphs
#+begin_notes
Rechenoperationen in ANN's werden typischerweise nicht in mathematischen Formeln angegeben, sondern in sog. computational Graphs.
#+end_notes

Typischerweise werden Operationen in artificial neural networks nicht mit mathematischen Formeln angegeben, sondern als Graph dargestellt.

*** Repräsentation
Jede Node in einem Graph $G$ repräsentiert eine mathematische Operation.\\

Beispielsweise:
- Matrix Multiplikation
- Addition
- Skalare Multiplikation
*** Addition Beispiel
$$
y = a+b
$$

[[./basic_graph.png]]
*** Komplexere Beispiele
$x=y+z$
$a=x\odot z$

[[./basic_graph_2.png]]

* Forward Propagation
Ein Layer in einem Feed-Forward Neural Network besteht aus folgenden Elementen:
- Inputs ($X$)
- Weights ($W$)
- Biases ($b$)
- Output ($a$)
** Formell
Um die Aktivierungen ($a$) eines Layers zu berechen können wir folgende Formel benutzen:
#+begin_quote
$a_L = \sigma(a_{L-1} w_L + b_L)$
#+end_quote
Der berechnete Vektor $a_L$ dient dem Layer $L+1$ als Input.
** Computational Graph
#+begin_notes
Backpropagation besteht letztendlich nur darin den Graph in die andere Richtung zu propagieren.
#+end_notes

$$
a = \sigma(a_{L-1}w_L+b)
$$

[[./forward_prop_graph.png]]

** Beispiel (XOR)
$W=\left[\begin{array}{ccc} 1 & 1 \\ 1 & 1 \end{array}\right]$ \\
$c=\left [\begin{array}{ccc} 0 \\ -1 \end{array} \right]$ \\
** Multiplizieren der Weights ($W$) und Inputs ($X$)
$$
XW=\left[\begin{array}{ccc} 0 & 0 \\ 0 & 1 \\ 1 & 0 \\ 1 & 1 \end{array} \right]
\left[\begin{array}{ccc} 1 & 1 \\ 1 & 1 \end{array}\right]=
\left[\begin{array}{ccc} 0 & 0 \\ 1 & 1 \\ 1 & 1 \\ 2 & 2 \end{array} \right]
$$

** Addieren des Bias Vektors ($c$)
$$
XW + c=
\left[\begin{array}{ccc} 0 & 0 \\ 1 & 1 \\ 1 & 1 \\ 2 & 2 \end{array} \right] +
\left(\begin{array}{ccc} 0 \\ -1 \end{array}\right)=
\left[\begin{array}{ccc} 0 & -1 \\ 1 & 0 \\ 1 & 0 \\ 2 & 1 \end{array} \right]
$$
** Aktivierungsfunktion (in diesem Fall $ReLU$)
#+begin_quote
$ReLU:= f(x)=max(0, x)$
#+end_quote
$$
relu(XW+c)=
relu(\left[\begin{array}{ccc} 0 & -1 \\ 1 & 0 \\ 1 & 0 \\ 2 & 1 \end{array} \right])=
\left[\begin{array}{ccc} 0 & 0 \\ 1 & 0 \\ 1 & 0 \\ 2 & 1 \end{array} \right]
$$

Die Aktivierungsfunktion wird auf jedes Element der Matrix ausgeführt.

** Output Layer
Multiplizieren der Output Matrix des ersten Layers mit den Weights des Output Layers ($w$).
$$
w= relu(XW+c)* \left[\begin{array}{ccc} 1 \\ -2 \end{array}\right]=
\left[\begin{array}{ccc} 0 & 0 \\ 1 & 0 \\ 1 & 0 \\ 2 & 1 \end{array} \right]*
\left[\begin{array}{ccc} 1 \\ -2 \end{array}\right]=
\left[\begin{array}{ccc} 0 \\ 1 \\ 1 \\ 0 \end{array}\right]
$$
** Predictions & Input
Input: $\left[\begin{array}{ccc} 0 & 0 \\ 0 & 1 \\ 1 & 0 \\ 1 & 1 \end{array} \right]$
Predictions: $\left[\begin{array}{ccc} 0 \\ 1 \\ 1 \\ 0 \end{array}\right]$

** Code Beispiel
#+BEGIN_SRC python
def forward(X):
    a = X
    for layer in L:
        a = h @ L.weights + L.bias
    return a
#+END_SRC

* Backpropagation
** Wozu brauchen wir den Backpropagation Algorithmus?
Ein fundamentaler Baustein, von neuralen Netzen.

Backpropagation ist kein Lernalgorithmus/Optimierungsalgorithmus, sondern aussschlißlich für die Generierung der Gradients jedes Layers zuständig.

Also suchen wir folgende Gradienten:
 - $\nabla_{b^k} J$
 - $\nabla_{w^k} J$

** Kettenregel
#+begin_notes
Da ein NN prinzipiell nur viele geschachtelte Funktionen sind ist die Kettenregel sehr nützlich um die Ableitungen für jede Funktion zu bestimmen.
#+end_notes

Die Kettenregel ist nützlich um Ableitungen aus schon bereits vorhandenen Ableitungen zu konstruieren.

$$y=g(x)\ und\ z=f(g(x))=f(y)$$

Dann besagt die Kettenregel: $\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}$
** Kettenregel als Graph

#+begin_notes
An der Formel $f'(f(f(w)))f'(f(w))f'(w)$ erkennt man, dass immer die Zwischenergebnisse aus jedem Schritt benötigt werden um die Korrekte Ableitung $\frac{\partial z}{\partial w}$.
#+end_notes

$$
x = f(w),\ y=f(x),\ z=f(y)
$$

[[./chain_rule_derriv.jpg]]

$$
\frac{\partial z}{\partial w}=
\frac{\partial z}{\partial y}
\frac{\partial y}{\partial x}
\frac{\partial x}{\partial w}
=
f'(y)f'(x)f'(w) \\
= f'(f(f(w)))f'(f(w))f'(w)
$$

** Anpassung der Forward Propagation
#+begin_notes
Wie davor gezeigt müssen wir nun Zwischenergebnisse aus der Forward Progagation speichern um im Anschluss effizient die Backpropagation durchführen zu können.
Eine Alternative ist bei *limitiertem Speicher* die Zwischenergebnisse immer neu zu evaluieren, wenn sie benötigt werden. (-> Höhere Laufzeit)
#+end_notes

Wir benötigen folgende Werte aus jedem Layer um den Backpropagation Algorithmus ausführen zu können.
- $a$ Aktivation Vektor
- $z$ Pre Activation Function Vektor
 
#+begin_quote
$f'(y)f'(x)f'(w)$: Speichern der Zwischenergebnisse in Variablen
$f'(f(f(w)))f'(f(w))f'(w)$: Neu Evaluierung der Zwischenergebnisse
#+end_quote
** Beschreibung des Algorithmus
*** Schritt 1
Forward Propagation ausführen.
*** Schritt 2
Wir berechnen den Gradienten der Cost Function $J$.
$J = \frac{1}{2} (y-X)^2 \rightarrow \nabla_y J = X - y$
*** Schritt 3
Erst müssen wir den Gradienten in Relation zu den pre activation function values berechnen.
#+begin_quote
$\nabla_{a^{k}} J = g \odot f'(a^{(k)})$
#+end_quote
mit $f'(x) := Ableitung\ der\ Aktivierungsfunktioin$
*** Schritt 4
Bias Gradienten berechnen.
#+begin_quote
$\nabla_{b^{k}} J = g$
#+end_quote
Weight Gradienten berechnen.
#+begin_quote
$\nabla_{w^k} J = ga^{k-1}$
#+end_quote
*** Schritt 5
$\nabla a^{k-1} J = w^kg$
** Graph
[[./backprop_derriv.jpg]]
** TODO Delta Rule
In neural Networks kann der Gradient Descent Algorithmus zu der sog. *Delta Rule* zusammengefasst werden.
#+begin_notes
$\lambda$ ist learning rate \\
$\alpha$ ist die aktivierungsfunktion \\
$z$ ist inputs * weights \\
#+end_notes

#+begin_quote
$$
\nabla w_{ji} = \lambda ( - a) \alpha'(z)a_{L-1}
$$
#+end_quote
** Praktisches Beispiel in Python
#+BEGIN_EXPORT html
<div class = "stretch">
     <iframe width="100%" height="100%" src="http://localhost:8888/lab"></iframe>
</div>
#+END_EXPORT

** Anmerkungen
In der Praxis werden keine Vektoren als Input Daten benutzt, sondern Matrizen (siehe ~XOR~ Beispiel).
$$
Input = \left[\begin{array}{ccc} 0 & 0 \\ 0 & 1 \\ 1 & 0 \\ 1 & 1 \end{array} \right]
$$

Wir erhalten nun auch mehrere Gradienten in Form einer Matrix. Wir können nun den Durchschnitt der Gradienten nutzen um unsere Weights anzupassen.

* General Back-propagation nach Ian Goodfellow
Bisher haben wir uns nur mit Back-propagation in Zusammenhang mit neuralen Netzwerken beschäftigt. \\
Back-propagation kann aber auch generell für andere Anwendungen eingesetzt werden.

** Operationen
#+begin_notes
Tensor kann beliebig viele Dimensionen zu haben.
#+end_notes
Wir betrachten einen computational Graph, jede Node in dem Graph repräsentiert eine Variable in Form eines Tensors.

*** Funktionen
#+begin_notes
~get_operation~ Beispiel bei einer Variable, die durch Matrix Multiplikation generiert wird, würde genau diese Operation zurück gegeben werden.
#+end_notes

Folgende Funktionen werden von Operationen implementiert:
- ~get_operation()~
- ~get_consumers()~ \\
  Gibt alle Variablen/Operationen zurück, die 'Kinder' von sich selber sind.
- ~get_inputs()~ \\
  Gibt alle Variablen/Operationen zurück, die 'Eltern' von sich selber sind.
- ~bprop()~ \\
  Muss bei jeder Operation implementiert werden.
** Algorithmus
Benötigt ist:
- die Menge aller Variablen $T$, deren Gradienten wir berechnen müssen
- den Graphen $G$
- die Variable $z$, die wir differenzieren wollen

*** Äussere Funktion
Wir definieren $G'$ als alle Variablen, die Vorfahren von $z$ sind oder Nachfahren von $T$. \\
In ~grad_table~ können wir Variablen Gradients zuweisen. \\

~grad_table[z] = 1~   (da $\frac{\partial z}{\partial z} = 1$)

*** Loop über alle Variablen, deren Gradienten wir berechnen müssen
In jedem Loop rufen wir die Funktion ~build_grad~ auf.
#+BEGIN_SRC python
for v in T:
    build_grad(v, G, G_1, grad_table)
return [grad_table[v] for v in T]
#+END_SRC

*** ~build_grad(v, G, G_1, grad_table)~
#+BEGIN_SRC python
def build_grad(v, G, G_1, grad_table):
    if v in grad_table: return grad_table[v]
    i = 1
    for c in get_consumers(V, G_1):
        op = get_operation(c)
        d = build_grad(c, G, G_1, grad_table)
        g[i] = op.bprop(get_inputs(c, G_1), v, d)
        i += 1
    g = sum(g)
    grad_table[v] = g
    return g
#+END_SRC

*** ~bprop~ Funktion
~op.bprop(inputs, X, G)~ \\
 \\
~inputs~: Liste an Inputs, die wir der Operation zur Verfügung stellen \\
~X~: Input, dessen Ableitung wir berechnen wollen \\
~G~: Gradient des Outputs der Operation

** Beispiel
*** Graph
[[./big_graph_1.png]]

*** Bestimmen der Ableitung $\frac{\partial u_1}{\partial u_4}$
[[./big_graph_2.png]]

** Generalisierbarkeit
Dadurch ist der Back-propagation Algorithmus sehr allgemein anwendbar. \\

Jede Operation ist für seine eigene Differenzierung verantwortlich und benötigt keine weiteren Informationen.

** Symbol to Number / Symbol to Symbol
Es existieren zwei verschiedene Möglichkeiten die Berechnungen der Gradients durchzuführen.

- Symbol to Number
- Symbol to Symbol

*** Symbol to Number
#+begin_notes
Methode die wir in vorherigen Beispielen verwendet waren.
#+end_notes

Die Input Variablen werden durch Zahlenwerte ersetzt und daraufhin (wie besprochen) alle nötigen Gradienten berechnet.

*** Symbol to Symbol
#+begin_notes
Symbol to Symbol benötigt zum differenzieren keine eigentlichen Zahlenwerte, sondern ersetzt diese durch Symbole. \\
Zusammengefasst kann man sagen, dass der Symbol to Number approach nur die Berechnungen ausführt die vom Symbol to Symbol als Graph erstellt werden.
#+end_notes

Beim der Symbol to Symbol Herangehensweise wird zuerst der Graph mit allen Ableitungen mit der Hilfe von symbolischen Werten konstruiert. \\
Später wird dann der Graph mit der Hilfe eines eigenen Algorithmus ausgewertet. \\

#+begin_quote
*Vorteil* \\
Ableitungen eines höheren Grads können berechnet werden, indem man den Back-propagation Algorithmus auf einen bereits abgeleiteten Graphen ausführt.
#+end_quote

* Historisches
#+begin_notes
Die Kettenregel stammt aus dem 17ten Jahrhundert.
#+end_notes

- Kettenregel stammt aus dem 17ten Jahrhundert (Leibniz, 1676). \\
- Lineare neurale Netzwerke Mitte des 20ten Jahrhunderts. \\
- Erfolgreiche Experimente mit Back-Propagation (1986)

* TODO Laufzeit
**
* Quellen
- ❤️ Deep Learning (Ian Goodfellow, Yoshua Bengio & Aaron Courville)
- https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c
- Wikipedia: https://en.wikipedia.org/wiki/Backpropagation
- Wikipedia: https://en.wikipedia.org/wiki/Delta_rule
