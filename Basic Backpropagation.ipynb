{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "\n",
    "\n",
    "![test](https://www.i2tutorials.com/wp-content/uploads/2019/09/Deep-learning-25-i2tutorials.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeakyReLU:  [1.0, 0.0, -5.0]  ->  [1.0, 0.0, -1.0]\n",
      "LeakyReLU (deriv):  [1.0, 0.0, -5.0]  ->  [1.0, 1.0, 0.2]\n"
     ]
    }
   ],
   "source": [
    "def relu(z, deriv=False):\n",
    "    activations = []\n",
    "    shape = z.shape\n",
    "    z = z.flatten()\n",
    "    if deriv:  # Return Derivative of Function\n",
    "        \n",
    "        for i in range(len(z)):  # Element Wise\n",
    "            if z[i] >= 0:\n",
    "                activations.append(1)\n",
    "            else:\n",
    "                activations.append(0.2)\n",
    "                \n",
    "        return np.array(activations).reshape(shape)\n",
    "    \n",
    "    for i in range(len(z)):\n",
    "        if z[i] > 0:\n",
    "            activations.append(z[i])\n",
    "        else:\n",
    "            activations.append(0.2 * z[i])\n",
    "            \n",
    "    return np.array(activations).reshape(shape)\n",
    "\n",
    "input = [1.,0.,-5.]\n",
    "print('LeakyReLU: ', input, ' -> ', list(relu(np.array(input))))\n",
    "\n",
    "print('LeakyReLU (deriv): ', input, ' -> ', list(relu(np.array(input), True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight & Bias Initialization\n",
    "\n",
    "Bias Values ($b$) are initialized with $0$.  \n",
    "Weight Values ($w$) are initialized with random values between $min$ and $max$.\n",
    "\n",
    "## Neural Network Structure\n",
    "\n",
    "**Neural Net Structure** `2 (Input) - 3 (Hidden) - 3 (Hidden) - 2 (Output)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "Weights:\n",
      " [[-0.67939492 -0.50053167 -0.15770879]\n",
      " [-0.98377615  0.20898538  0.7700084 ]] (Shape: (2, 3))\n",
      "Bias: \n",
      "[0. 0. 0.] (Shape: (3,))\n",
      "\n",
      "Layer 1:\n",
      "Weights:\n",
      " [[ 0.74120638  0.73591049  0.56212497]\n",
      " [ 0.14326917 -0.79721194  0.18521068]\n",
      " [-0.92260994  0.22428948  0.2028797 ]] (Shape: (3, 3))\n",
      "Bias: \n",
      "[0. 0. 0.] (Shape: (3,))\n",
      "\n",
      "Layer 2:\n",
      "Weights:\n",
      " [[ 0.28725166  0.88825793]\n",
      " [-0.38772979 -0.46299148]\n",
      " [ 0.20082469 -0.44355096]] (Shape: (3, 2))\n",
      "Bias: \n",
      "[0. 0.] (Shape: (2,))\n",
      "\n",
      "Layer 3:\n",
      "Weights:\n",
      " [[-0.31457097]\n",
      " [ 0.21298103]] (Shape: (2, 1))\n",
      "Bias: \n",
      "[0.] (Shape: (1,))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def initialize(min=0, max=1, do_print=True):\n",
    "    global w, b\n",
    "    w = [\n",
    "            max * np.random.uniform(min, max, (2, 3)),\n",
    "            max * np.random.uniform(min, max, (3, 3)),\n",
    "            max * np.random.uniform(min, max, (3, 2)),\n",
    "            max * np.random.uniform(min, max, (2, 1))\n",
    "        ]\n",
    "    b = [\n",
    "        np.array(np.zeros(3)),\n",
    "        np.array(np.zeros(3)),\n",
    "        np.array(np.zeros(2)),\n",
    "        np.array(np.zeros(1))\n",
    "    ]\n",
    "    if do_print:\n",
    "        for i in range(len(b)): print(f'Layer {i}:\\nWeights:\\n {w[i]} (Shape: {w[i].shape})\\nBias: \\n{b[i]} (Shape: {b[i].shape})\\n')    \n",
    "\n",
    "w, b = [], []\n",
    "\n",
    "initialize(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "List $a$ holds each layers activation vector.  \n",
    "List $z$ holds each layers pre nonlinearity vector.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "For each layer $L$, starting with $L_0$ we multiply the $h$ vector with the weight matrix $w$.\n",
    "\n",
    "$$\n",
    "w = \\left[ \\begin{array}{rrr}\n",
    "1.3 & 0.2 \\\\                                              \n",
    "0.1 & 1.4 \\\\\n",
    "1.2 & 0 \\\\\n",
    "\\end{array}\\right] \\ \\ \\ \\ \\ \\ \\ \n",
    "h = \\left( \\begin{array}{rrr}\n",
    "1.3 \\\\                                              \n",
    "0.1 \\\\\n",
    "1.2 \\\\\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h:  [[0 1]] (1, 2)\n",
      "Layer 0 | [[0 1]] dot [[0.2801375  0.07565901 0.5413035 ]\n",
      " [0.16905846 0.85158748 0.65993038]]\n",
      "Layer 1 | [[0.16905846 0.85158748 0.65993038]] dot [[0.5753936  0.77522391 0.22571497]\n",
      " [0.19983397 0.81690688 0.29932804]\n",
      " [0.98716794 0.81983226 0.93343491]]\n",
      "Layer 2 | [[0.91891337 1.36775804 0.90906509]] dot [[0.08386694 0.37386451]\n",
      " [0.78246001 0.19594556]\n",
      " [0.74067649 0.41443656]]\n",
      "Layer 3 | [[1.82060556 0.98830502]] dot [[0.6409151 ]\n",
      " [0.33988443]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.50276308]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, z = [], []  # Global Variables\n",
    "\n",
    "initialize(do_print=False)  # Weight & Bias Initialization\n",
    "\n",
    "def forward_prop(X, do_print=False):\n",
    "    h = np.array([X])\n",
    "    if do_print: print('h: ', h, h.shape)\n",
    "    global a, z\n",
    "    a,z  = [], []\n",
    "    for i in range(len(w)):\n",
    "        if do_print: print(f'Layer {i} | {h} dot {w[i]}')\n",
    "        h = h @ w[i] # weigt * input\n",
    "        h = h + b[i] # bias add\n",
    "        z.append(h)\n",
    "        h = relu(h) # Activation Function\n",
    "        a.append(h)\n",
    "    return h\n",
    "\n",
    "forward_prop(np.array([0,1]), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back-propagation\n",
    "\n",
    "\n",
    "$g = loss'(X,y)$\n",
    "\n",
    "## for each layer\n",
    "\n",
    "### Step 1 ($a$ to $z$)\n",
    "\n",
    "$g = relu'(z)$\n",
    "\n",
    "\n",
    "\n",
    "### Step 2 ($z$ to $W$)\n",
    "\n",
    "$g = relu'(z) * a_{L-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17367607]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.34140562]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "\n",
    "initialize(do_print=False)\n",
    "\n",
    "def back_prop(X, y, print_loss=False):\n",
    "    global a, z, w, b\n",
    "\n",
    "    loss = 0.5*(y - X)**2  #  Calculate Loss Value\n",
    "    g = (X - y).reshape(1,-1).T  # Loss Function Derivative\n",
    "    \n",
    "    if print_loss:  # Print Loss \n",
    "        print(\"Loss: \", (y - X)**2)\n",
    "    \n",
    "    n_weights, n_bias = [], []\n",
    "    \n",
    "    for x in range(len(w)):  # Iterate through NN Layers\n",
    "        i = len(b) - 1 - x  # Reverse Index\n",
    "\n",
    "        # Activation Function Derivative \n",
    "        g = g * relu(z[i], True)  # Activation Function Derivative\n",
    "        \n",
    "        # Derivative with respect to weight\n",
    "        if i-1 < 0: w_der = y.reshape(1,-1).T\n",
    "        else: w_der = a[i-1].reshape(1,-1).T  # Previous Layer Activation\n",
    "        \n",
    "        # Change in Weights & Bias\n",
    "        n_weights.append(w[i] - learning_rate * (w_der @ g))\n",
    "        n_bias.append(b[i] - learning_rate * g)\n",
    "        \n",
    "        # Deriv for next Layer\n",
    "        g = g @ w[i].T \n",
    "    \n",
    "    # Updating Weights\n",
    "    w = list(reversed(n_weights))\n",
    "    b = list(reversed(n_bias))\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "\n",
    "X = forward_prop(np.array([0,1]))\n",
    "print(X)\n",
    "back_prop(X, np.array([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function\n",
    "\n",
    "## Adding 2 Numbers\n",
    "\n",
    "We train a simple Function $f^*(x)=x_1+x_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss:  [0.01989923]\n",
      "Epoch 1 | Loss:  [0.00459328]\n",
      "Epoch 2 | Loss:  [0.00509175]\n",
      "Epoch 3 | Loss:  [0.00420636]\n",
      "Epoch 4 | Loss:  [0.00336174]\n",
      "Epoch 5 | Loss:  [0.00339214]\n",
      "Epoch 6 | Loss:  [0.00322784]\n",
      "Epoch 7 | Loss:  [0.0029589]\n",
      "Epoch 8 | Loss:  [0.0029363]\n",
      "Epoch 9 | Loss:  [0.00250332]\n",
      "Epoch 10 | Loss:  [0.00237086]\n",
      "Epoch 11 | Loss:  [0.00248545]\n",
      "Epoch 12 | Loss:  [0.00248772]\n",
      "Epoch 13 | Loss:  [0.00181658]\n",
      "Epoch 14 | Loss:  [0.00171092]\n",
      "Epoch 15 | Loss:  [0.00197547]\n",
      "Epoch 16 | Loss:  [0.00216922]\n",
      "Epoch 17 | Loss:  [0.00155648]\n",
      "Epoch 18 | Loss:  [0.00182772]\n",
      "Epoch 19 | Loss:  [0.0016884]\n",
      "\n",
      "Input:\n",
      "[[0.2 0.3]\n",
      " [0.7 0.2]\n",
      " [1.  0. ]\n",
      " [1.  1. ]] \n",
      "Output:\n",
      "[[0.50381916]\n",
      " [0.8651642 ]\n",
      " [0.93299112]\n",
      " [1.99713469]]\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "def train(epochs, size=100):\n",
    "    for i in range(epochs):\n",
    "        tmp = 0.0\n",
    "        for x in range(size):\n",
    "            y1 = randint(0,1)  # X1\n",
    "            y2 = randint(0,1)  # X2\n",
    "            tmp += back_prop(forward_prop(np.array([y1,y2])), np.array([y1+y2]))[0]\n",
    "        print(f'Epoch {i} | Loss: ', tmp/size)\n",
    "\n",
    "initialize(do_print=False)\n",
    "train(20, 100)\n",
    "\n",
    "print('\\nInput:')\n",
    "print(np.array([[0.2,0.3], \n",
    "     [0.7, 0.2], \n",
    "     [1.,0.], \n",
    "     [1,1]]), '\\nOutput:')\n",
    "print(forward_prop(np.array([[0.2,0.3], \n",
    "                             [0.7, 0.2], \n",
    "                             [1.,0.], \n",
    "                             [1,1]]))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [0.11687813]\n",
      "Loss:  [0.0383725]\n",
      "Loss:  [0.02859404]\n",
      "Loss:  [2.17582337e-06]\n",
      "[[1.00016374]]\n",
      "[[0.99988356]]\n",
      "[[0.00065774]]\n",
      "[[-0.0034675]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11dfcecd0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [\n",
    "    [0,1],\n",
    "    [0,0],\n",
    "    [1,0],\n",
    "    [1,1]\n",
    "]   # Design Matrix \n",
    "\n",
    "y = [1,0,1,0]    # Labels\n",
    "\n",
    "initialize(-1, 1, do_print=False)  # Reinitialize\n",
    "\n",
    "def train(epochs, size=100):\n",
    "    l = []\n",
    "    for i in range(epochs):\n",
    "        tmp = 0.0\n",
    "        for x in range(size):\n",
    "            y1 = randint(0,3)\n",
    "            tmp += back_prop(forward_prop(np.array(X[y1])), np.array(y[y1]))[0]\n",
    "        l.append(tmp/size)\n",
    "        print(\"Loss: \", tmp/size)\n",
    "        if tmp/size < 0.0001: break\n",
    "    return np.array(l).flatten()\n",
    "\n",
    "loss_over_time =  train(300,600)\n",
    "\n",
    "print(forward_prop(np.array([1,0])))\n",
    "print(forward_prop(np.array([0,1])))\n",
    "print(forward_prop(np.array([0,0])))\n",
    "print(forward_prop(np.array([1,1])))\n",
    "\n",
    "plt.plot(loss_over_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Backpropagation Implementation\n",
    "Exmaple: Learning XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Loss: [0.128]\n",
      "[Epoch 1] Loss: [0.117]\n",
      "[Epoch 2] Loss: [0.092]\n",
      "[Epoch 3] Loss: [0.102]\n",
      "[Epoch 4] Loss: [0.108]\n",
      "[Epoch 5] Loss: [0.111]\n",
      "[Epoch 6] Loss: [0.105]\n",
      "[Epoch 7] Loss: [0.104]\n",
      "[Epoch 8] Loss: [0.103]\n",
      "[Epoch 9] Loss: [0.109]\n",
      "[Epoch 10] Loss: [0.106]\n",
      "[Epoch 11] Loss: [0.092]\n",
      "[Epoch 12] Loss: [0.091]\n",
      "[Epoch 13] Loss: [0.09]\n",
      "[Epoch 14] Loss: [0.109]\n",
      "[Epoch 15] Loss: [0.121]\n",
      "[Epoch 16] Loss: [0.118]\n",
      "[Epoch 17] Loss: [0.009]\n",
      "[Epoch 18] Loss: [1.596e-05]\n",
      "[[[9.993e-01]\n",
      "  [1.001e+00]\n",
      "  [5.109e-04]\n",
      "  [2.367e-05]]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "\n",
    "def backprop_entry(X, y, print_loss=False):\n",
    "    global a, z, w, b, n_weights, n_bias\n",
    "    n_weights, n_bias = [], []\n",
    "    backprop_rec(0, X, y)\n",
    "    \n",
    "    # Update Weights\n",
    "    w = list(reversed(n_weights))\n",
    "    b = list(reversed(n_bias))\n",
    "    return 0.5*(y - X)**2  # Return Loss\n",
    "\n",
    "\n",
    "def backprop_rec(i, X, y):\n",
    "    global a, z, w, b, n_weights, n_bias\n",
    "\n",
    "    # Base Case\n",
    "    if i+1 > len(w): return (X - y).reshape(1,-1).T\n",
    "    \n",
    "    g = backprop_rec(i+1, X, y) * relu(z[i], True)  # Get Next Layer Derivative\n",
    "    \n",
    "    # Derivative with respect to weight [1xn]  \n",
    "    if i-1 < 0: w_der = y.reshape(1,-1).T  # Input Matrix\n",
    "    else: w_der = a[i-1].reshape(1,-1).T  # Previous Layer Activation\n",
    "\n",
    "    # Save change in weights\n",
    "    n_weights.append(w[i] - learning_rate * (w_der @ g))\n",
    "    n_bias.append(b[i] - learning_rate * g)\n",
    "    return g @ w[i].T \n",
    "\n",
    "def train_rec(epochs, size=100, threshold=0.0001):\n",
    "    l = []\n",
    "    for i in range(epochs):\n",
    "        sum_loss = 0.0\n",
    "        for x in range(size):\n",
    "            y1 = randint(0,3)\n",
    "            \n",
    "            sum_loss += backprop_entry(forward_prop(np.array(X[y1])), np.array(y[y1]))[0]\n",
    "        l.append(sum_loss/size)\n",
    "        print(f'[Epoch {i}] Loss: {l[-1]}')\n",
    "        if l[-1] < threshold or l[-1] != l[-1]: break\n",
    "    return np.array(l).flatten()\n",
    "\n",
    "# Reinitialize Weights & Bias\n",
    "initialize(-1, 1, do_print=False)\n",
    "\n",
    "loss_over_time = train_rec(20,1000)\n",
    "\n",
    "\n",
    "print(forward_prop(np.array([[1,0], \n",
    "                             [0,1], \n",
    "                             [1,1], \n",
    "                             [0,0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "name": "Minimal Backprop.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
