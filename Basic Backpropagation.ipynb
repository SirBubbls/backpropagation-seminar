{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "\n",
    "\n",
    "![test](https://www.i2tutorials.com/wp-content/uploads/2019/09/Deep-learning-25-i2tutorials.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeakyReLU:  [1.0, 0.0, -5.0]  ->  [1.0, 0.0, -1.0]\n",
      "LeakyReLU (deriv):  [1.0, 0.0, -5.0]  ->  [1.0, 1.0, 0.2]\n"
     ]
    }
   ],
   "source": [
    "def relu(z, deriv=False):\n",
    "    activations = []\n",
    "    shape = z.shape\n",
    "    z = z.flatten()\n",
    "    if deriv:  # Return Derivative of Function\n",
    "        \n",
    "        for i in range(len(z)):  # Element Wise\n",
    "            if z[i] >= 0:\n",
    "                activations.append(1)\n",
    "            else:\n",
    "                activations.append(0.2)\n",
    "                \n",
    "        return np.array(activations).reshape(shape)\n",
    "    \n",
    "    for i in range(len(z)):\n",
    "        if z[i] > 0:\n",
    "            activations.append(z[i])\n",
    "        else:\n",
    "            activations.append(0.2 * z[i])\n",
    "            \n",
    "    return np.array(activations).reshape(shape)\n",
    "\n",
    "input = [1.,0.,-5.]\n",
    "print('LeakyReLU: ', input, ' -> ', list(relu(np.array(input))))\n",
    "\n",
    "print('LeakyReLU (deriv): ', input, ' -> ', list(relu(np.array(input), True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight & Bias Initialization\n",
    "\n",
    "Bias Values ($b$) are initialized with $0$.  \n",
    "Weight Values ($w$) are initialized with random values between $min$ and $max$.\n",
    "\n",
    "## Neural Network Structure\n",
    "\n",
    "**Neural Net Structure** `2 (Input) - 3 (Hidden) - 3 (Hidden) - 2 (Output)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "Weights:\n",
      " [[-0.46756736 -0.01183008 -0.91218701]\n",
      " [-0.08040447  0.05467754 -0.87948642]] (Shape: (2, 3))\n",
      "Bias: \n",
      "[0. 0. 0.] (Shape: (3,))\n",
      "\n",
      "Layer 1:\n",
      "Weights:\n",
      " [[-0.84726787 -0.72224271 -0.79410495]\n",
      " [ 0.81371569 -0.85203241 -0.43781362]\n",
      " [-0.37662838  0.12766053  0.40477439]] (Shape: (3, 3))\n",
      "Bias: \n",
      "[0. 0. 0.] (Shape: (3,))\n",
      "\n",
      "Layer 2:\n",
      "Weights:\n",
      " [[-0.1389832  -0.93693281]\n",
      " [-0.519337   -0.98862196]\n",
      " [ 0.7584379   0.31359785]] (Shape: (3, 2))\n",
      "Bias: \n",
      "[0. 0.] (Shape: (2,))\n",
      "\n",
      "Layer 3:\n",
      "Weights:\n",
      " [[-0.35673993]\n",
      " [ 0.60131043]] (Shape: (2, 1))\n",
      "Bias: \n",
      "[0.] (Shape: (1,))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def initialize(min=0, max=1, do_print=True):\n",
    "    global w, b\n",
    "    w = [\n",
    "            max * np.random.uniform(min, max, (2, 3)),\n",
    "            max * np.random.uniform(min, max, (3, 3)),\n",
    "            max * np.random.uniform(min, max, (3, 2)),\n",
    "            max * np.random.uniform(min, max, (2, 1))\n",
    "        ]\n",
    "    b = [\n",
    "        np.array(np.zeros(3)),\n",
    "        np.array(np.zeros(3)),\n",
    "        np.array(np.zeros(2)),\n",
    "        np.array(np.zeros(1))\n",
    "    ]\n",
    "    if do_print:\n",
    "        for i in range(len(b)): print(f'Layer {i}:\\nWeights:\\n {w[i]} (Shape: {w[i].shape})\\nBias: \\n{b[i]} (Shape: {b[i].shape})\\n')    \n",
    "\n",
    "w, b = [], []\n",
    "\n",
    "initialize(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "List $a$ holds each layers activation vector.  \n",
    "List $z$ holds each layers pre nonlinearity vector.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "For each layer $L$, starting with $L_0$ we multiply the $h$ vector with the weight matrix $w$.\n",
    "\n",
    "$$\n",
    "w = \\left[ \\begin{array}{rrr}\n",
    "1.3 & 0.2 \\\\                                              \n",
    "0.1 & 1.4 \\\\\n",
    "1.2 & 0 \\\\\n",
    "\\end{array}\\right] \\ \\ \\ \\ \\ \\ \\ \n",
    "h = \\left( \\begin{array}{rrr}\n",
    "1.3 \\\\                                              \n",
    "0.1 \\\\\n",
    "1.2 \\\\\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h:  [[0 1]] (1, 2)\n",
      "Layer 0 | [[0 1]] dot [[0.45335439 0.91881761 0.41558788]\n",
      " [0.06677665 0.27624812 0.16607905]]\n",
      "Layer 1 | [[0.06677665 0.27624812 0.16607905]] dot [[0.67902838 0.05591359 0.36053027]\n",
      " [0.60157524 0.64274338 0.03346819]\n",
      " [0.08121304 0.67499162 0.3005757 ]]\n",
      "Layer 2 | [[0.22501505 0.29339234 0.08323985]] dot [[0.20482315 0.37049071]\n",
      " [0.6399956  0.58369372]\n",
      " [0.19981877 0.41846314]]\n",
      "Layer 3 | [[0.25049098 0.28945006]] dot [[0.33588539]\n",
      " [0.87499975]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.33740499]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, z = [], []  # Global Variables\n",
    "\n",
    "initialize(do_print=False)  # Weight & Bias Initialization\n",
    "\n",
    "def forward_prop(X, do_print=False):\n",
    "    h = np.array([X])\n",
    "    if do_print: print('h: ', h, h.shape)\n",
    "    global a, z\n",
    "    a,z  = [], []\n",
    "    for i in range(len(w)):\n",
    "        if do_print: print(f'Layer {i} | {h} dot {w[i]}')\n",
    "        h = h @ w[i] # weigt * input\n",
    "        h = h + b[i] # bias add\n",
    "        z.append(h)\n",
    "        h = relu(h) # Activation Function\n",
    "        a.append(h)\n",
    "    return h\n",
    "\n",
    "forward_prop(np.array([0,1]), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back-propagation\n",
    "\n",
    "\n",
    "$g = loss'(X,y)$\n",
    "\n",
    "## for each layer\n",
    "\n",
    "### Step 1 ($a$ to $z$)\n",
    "\n",
    "$g = relu'(z)$\n",
    "\n",
    "\n",
    "\n",
    "### Step 2 ($z$ to $W$)\n",
    "\n",
    "$g = relu'(z) * a_{L-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.62214719]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.31568075]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "\n",
    "initialize(do_print=False)\n",
    "\n",
    "def back_prop(X, y, print_loss=False):\n",
    "    global a, z, w, b\n",
    "\n",
    "    loss = 0.5*(y - X)**2  #  Calculate Loss Value\n",
    "    g = (X - y).reshape(1,-1).T  # Loss Function Derivative\n",
    "    \n",
    "    if print_loss:  # Print Loss \n",
    "        print(\"Loss: \", (y - X)**2)\n",
    "    \n",
    "    n_weights, n_bias = [], []\n",
    "    \n",
    "    for x in range(len(w)):  # Iterate through NN Layers\n",
    "        i = len(b) - 1 - x  # Reverse Index\n",
    "\n",
    "        # Activation Function Derivative \n",
    "        g = g * relu(z[i], True)  # Activation Function Derivative\n",
    "        \n",
    "        # Derivative with respect to weight\n",
    "        if i-1 < 0: w_der = y.reshape(1,-1).T\n",
    "        else: w_der = a[i-1].reshape(1,-1).T  # Previous Layer Activation\n",
    "        \n",
    "        # Change in Weights & Bias\n",
    "        n_weights.append(w[i] - learning_rate * (w_der @ g))\n",
    "        n_bias.append(b[i] - learning_rate * g)\n",
    "        \n",
    "        # Deriv for next Layer\n",
    "        g = g @ w[i].T \n",
    "    \n",
    "    # Updating Weights\n",
    "    w = list(reversed(n_weights))\n",
    "    b = list(reversed(n_bias))\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "\n",
    "X = forward_prop(np.array([0,1]))\n",
    "print(X)\n",
    "back_prop(X, np.array([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function\n",
    "\n",
    "## Adding 2 Numbers\n",
    "\n",
    "We train a simple Function $f^*(x)=x_1+x_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss:  [0.0530934]\n",
      "Epoch 1 | Loss:  [0.01255926]\n",
      "Epoch 2 | Loss:  [0.00986967]\n",
      "Epoch 3 | Loss:  [0.00538713]\n",
      "Epoch 4 | Loss:  [0.00285176]\n",
      "Epoch 5 | Loss:  [0.0015905]\n",
      "Epoch 6 | Loss:  [0.00094948]\n",
      "Epoch 7 | Loss:  [0.00054639]\n",
      "Epoch 8 | Loss:  [0.00026938]\n",
      "Epoch 9 | Loss:  [0.00013713]\n",
      "Epoch 10 | Loss:  [7.40509493e-05]\n",
      "Epoch 11 | Loss:  [3.18356765e-05]\n",
      "Epoch 12 | Loss:  [1.76461019e-05]\n",
      "Epoch 13 | Loss:  [7.79533929e-06]\n",
      "Epoch 14 | Loss:  [3.81309046e-06]\n",
      "Epoch 15 | Loss:  [1.77681953e-06]\n",
      "Epoch 16 | Loss:  [7.59270445e-07]\n",
      "Epoch 17 | Loss:  [3.17063515e-07]\n",
      "Epoch 18 | Loss:  [1.62483549e-07]\n",
      "Epoch 19 | Loss:  [7.47842135e-08]\n",
      "\n",
      "Input:\n",
      "[[0.2 0.3]\n",
      " [0.7 0.2]\n",
      " [1.  0. ]\n",
      " [1.  1. ]] \n",
      "Output:\n",
      "[[0.4997125 ]\n",
      " [0.89957076]\n",
      " [0.99942117]\n",
      " [1.9998822 ]]\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "def train(epochs, size=100):\n",
    "    for i in range(epochs):\n",
    "        tmp = 0.0\n",
    "        for x in range(size):\n",
    "            y1 = randint(0,1)  # X1\n",
    "            y2 = randint(0,1)  # X2\n",
    "            tmp += back_prop(forward_prop(np.array([y1,y2])), np.array([y1+y2]))[0]\n",
    "        print(f'Epoch {i} | Loss: ', tmp/size)\n",
    "\n",
    "initialize(do_print=False)\n",
    "train(20, 100)\n",
    "\n",
    "print('\\nInput:')\n",
    "print(np.array([[0.2,0.3], \n",
    "     [0.7, 0.2], \n",
    "     [1.,0.], \n",
    "     [1,1]]), '\\nOutput:')\n",
    "print(forward_prop(np.array([[0.2,0.3], \n",
    "                             [0.7, 0.2], \n",
    "                             [1.,0.], \n",
    "                             [1,1]]))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [0.13291364]\n",
      "Loss:  [0.12880271]\n",
      "Loss:  [0.12492207]\n",
      "Loss:  [0.10460824]\n",
      "Loss:  [0.06938157]\n",
      "Loss:  [0.01590086]\n",
      "Loss:  [5.21344412e-05]\n",
      "[[0.99994306]]\n",
      "[[0.9999507]]\n",
      "[[1.54262188e-05]]\n",
      "[[0.00039161]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12163b4d0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [\n",
    "    [0,1],\n",
    "    [0,0],\n",
    "    [1,0],\n",
    "    [1,1]\n",
    "]   # Design Matrix\n",
    "\n",
    "y = [1,0,1,0]    # Labels\n",
    "\n",
    "initialize(-1, 1, do_print=False)  # Reinitialize\n",
    "\n",
    "def train(epochs, size=100):\n",
    "    l = []\n",
    "    for i in range(epochs):\n",
    "        tmp = 0.0\n",
    "        for x in range(size):\n",
    "            y1 = randint(0,3)\n",
    "            tmp += back_prop(forward_prop(np.array(X[y1])), np.array(y[y1]))[0]\n",
    "        l.append(tmp/size)\n",
    "        print(\"Loss: \", tmp/size)\n",
    "        if tmp/size < 0.0001: break\n",
    "    return np.array(l).flatten()\n",
    "\n",
    "loss_over_time =  train(300,600)\n",
    "\n",
    "print(forward_prop(np.array([1,0])))\n",
    "print(forward_prop(np.array([0,1])))\n",
    "print(forward_prop(np.array([0,0])))\n",
    "print(forward_prop(np.array([1,1])))\n",
    "\n",
    "plt.plot(loss_over_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Backpropagation Implementation\n",
    "Exmaple: Learning XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Loss: [0.072]\n",
      "[Epoch 1] Loss: [0.]\n",
      "[Epoch 2] Loss: [9.15e-11]\n",
      "[[[1.000e+00]\n",
      "  [1.000e+00]\n",
      "  [9.527e-08]\n",
      "  [9.566e-09]]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "\n",
    "def backprop_entry(X, y, print_loss=False):\n",
    "    global a, z, w, b, n_weights, n_bias\n",
    "    n_weights, n_bias = [], []\n",
    "    backprop_rec(0, X, y)\n",
    "    \n",
    "    # Update Weights\n",
    "    w = list(reversed(n_weights))\n",
    "    b = list(reversed(n_bias))\n",
    "    return 0.5*(y - X)**2  # Return Loss\n",
    "\n",
    "\n",
    "def backprop_rec(i, X, y):\n",
    "    global a, z, w, b, n_weights, n_bias\n",
    "\n",
    "    # Base Case\n",
    "    if i+1 > len(w): return (X - y).reshape(1,-1).T\n",
    "    \n",
    "    g = backprop_rec(i+1, X, y) * relu(z[i], True)  # Get Next Layer Derivative\n",
    "    \n",
    "    # Derivative with respect to weight [1xn]  \n",
    "    if i-1 < 0: prev_a = y.reshape(1,-1).T  # Input Matrix\n",
    "    else: prev_a = a[i-1].reshape(1,-1).T  # Previous Layer Activation\n",
    "\n",
    "    # Save change in weights\n",
    "    n_weights.append(w[i] - learning_rate * (prev_a @ g))\n",
    "    n_bias.append(b[i] - learning_rate * g)\n",
    "    return g @ w[i].T \n",
    "\n",
    "def train_rec(epochs, size=100, threshold=0.0001):\n",
    "    l = []\n",
    "    for i in range(epochs):\n",
    "        sum_loss = 0.0\n",
    "        for x in range(size):\n",
    "            y1 = randint(0,3)\n",
    "            \n",
    "            sum_loss += backprop_entry(forward_prop(np.array(X[y1])), np.array(y[y1]))[0]\n",
    "        l.append(sum_loss/size)\n",
    "        print(f'[Epoch {i}] Loss: {l[-1]}')\n",
    "        if l[-1] < threshold or l[-1] != l[-1]: break\n",
    "    return np.array(l).flatten()\n",
    "\n",
    "# Reinitialize Weights & Bias\n",
    "initialize(-1, 1, do_print=False)\n",
    "\n",
    "loss_over_time = train_rec(20,1000)\n",
    "\n",
    "\n",
    "print(forward_prop(np.array([[1,0], \n",
    "                             [0,1], \n",
    "                             [1,1], \n",
    "                             [0,0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "name": "Minimal Backprop.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
